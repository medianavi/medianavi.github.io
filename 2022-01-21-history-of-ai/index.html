<!DOCTYPE html><html lang="ko"><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><meta name="generator" content="Gatsby 4.25.9"/><meta name="description" content="미디어나비의 CTO인 Justa가 BERT와 인공지능의 역사에 대해 설명합니다." data-gatsby-head="true"/><meta property="og:title" content="인공지능의 역사에서 BERT 이해하기" data-gatsby-head="true"/><meta property="og:description" content="미디어나비의 CTO인 Justa가 BERT와 인공지능의 역사에 대해 설명합니다." data-gatsby-head="true"/><meta property="og:type" content="website" data-gatsby-head="true"/><meta name="twitter:card" content="summary" data-gatsby-head="true"/><meta name="twitter:creator" content="" data-gatsby-head="true"/><meta name="twitter:title" content="인공지능의 역사에서 BERT 이해하기" data-gatsby-head="true"/><meta name="twitter:description" content="미디어나비의 CTO인 Justa가 BERT와 인공지능의 역사에 대해 설명합니다." data-gatsby-head="true"/><style data-href="/styles.6d4a2b068fe0b7cfb545.css" data-identity="gatsby-global-css">@import url(https://cdn.jsdelivr.net/gh/orioncactus/pretendard/dist/web/static/pretendard-dynamic-subset.css);
/*! normalize.css v8.0.1 | MIT License | github.com/necolas/normalize.css */html{-webkit-text-size-adjust:100%;line-height:1.15}body{margin:0}main{display:block}h1{font-size:2em;margin:.67em 0}hr{box-sizing:content-box;height:0;overflow:visible}pre{font-family:monospace,monospace;font-size:1em}a{background-color:transparent}abbr[title]{border-bottom:none;text-decoration:underline;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}b,strong{font-weight:bolder}code,kbd,samp{font-family:monospace,monospace;font-size:1em}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}img{border-style:none}button,input,optgroup,select,textarea{font-family:inherit;font-size:100%;line-height:1.15;margin:0}button,input{overflow:visible}button,select{text-transform:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]::-moz-focus-inner,[type=reset]::-moz-focus-inner,[type=submit]::-moz-focus-inner,button::-moz-focus-inner{border-style:none;padding:0}[type=button]:-moz-focusring,[type=reset]:-moz-focusring,[type=submit]:-moz-focusring,button:-moz-focusring{outline:1px dotted ButtonText}fieldset{padding:.35em .75em .625em}legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal}progress{vertical-align:baseline}textarea{overflow:auto}[type=checkbox],[type=radio]{box-sizing:border-box;padding:0}[type=number]::-webkit-inner-spin-button,[type=number]::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}[type=search]::-webkit-search-decoration{-webkit-appearance:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}details{display:block}summary{display:list-item}[hidden]{display:none}:lang(ko) h1,:lang(ko) h2,:lang(ko) h3,:lang(ko) h4{word-break:keep-all}:root{--maxWidth-none:"none";--maxWidth-xs:20rem;--maxWidth-sm:24rem;--maxWidth-md:28rem;--maxWidth-lg:32rem;--maxWidth-xl:36rem;--maxWidth-2xl:42rem;--maxWidth-3xl:48rem;--maxWidth-4xl:56rem;--maxWidth-full:"100%";--maxWidth-wrapper:var(--maxWidth-2xl);--spacing-px:"1px";--spacing-0:0;--spacing-1:0.25rem;--spacing-2:0.5rem;--spacing-3:0.75rem;--spacing-4:1rem;--spacing-5:1.25rem;--spacing-6:1.5rem;--spacing-8:2rem;--spacing-10:2.5rem;--spacing-12:3rem;--spacing-16:4rem;--spacing-20:5rem;--spacing-24:6rem;--spacing-32:8rem;--fontFamily-sans:Pretendard,system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans","Apple SD Gothic Neo","Noto Sans KR","Malgun Gothic",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";--fontFamily-serif:"Merriweather","Georgia",Cambria,"Times New Roman",Times,serif;--font-body:var(--fontFamily-sans);--font-heading:var(--fontFamily-sans);--fontWeight-normal:400;--fontWeight-medium:500;--fontWeight-semibold:600;--fontWeight-bold:700;--fontWeight-extrabold:800;--fontWeight-black:900;--fontSize-root:16px;--lineHeight-none:1;--lineHeight-tight:1.1;--lineHeight-normal:1.5;--lineHeight-relaxed:1.625;--fontSize-0:0.833rem;--fontSize-1:1rem;--fontSize-2:1.2rem;--fontSize-3:1.44rem;--fontSize-4:1.728rem;--fontSize-5:2.074rem;--fontSize-6:2.488rem;--fontSize-7:2.986rem;--color-primary:#e46b00;--color-text-light:#2e353f;--color-heading:#000;--color-heading-black:#000;--color-accent:#d1dce5;--color-key:#2238ce;--color-base:#ededed;--color-text:#000;--base-margin:30px}*,:after,:before{box-sizing:border-box}html{-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;font-size:var(--fontSize-root);line-height:var(--lineHeight-normal);overflow-y:scroll}body{background-color:var(--color-base);color:var(--color-text);font-family:var(--font-body);font-size:var(--fontSize-1)}footer{padding:var(--spacing-6) var(--spacing-0)}hr{background:var(--color-accent);border:0;height:1px}h1,h2,h3,h4,h5,h6{font-family:var(--font-heading);letter-spacing:-.025em;line-height:var(--lineHeight-tight);margin-bottom:var(--spacing-6);margin-top:var(--spacing-12)}h2,h3,h4,h5,h6{font-weight:var(--fontWeight-bold)}h1,h2,h3,h4,h5,h6{color:var(--color-text)}h1{font-size:var(--fontSize-6);font-weight:var(--fontWeight-black)}h2{font-size:var(--fontSize-5)}h3{font-size:var(--fontSize-4)}h4{font-size:var(--fontSize-3)}h5{font-size:var(--fontSize-2)}h6{font-size:var(--fontSize-1)}h1>a,h2>a,h3>a,h4>a,h5>a,h6>a{color:inherit;text-decoration:none}p{--baseline-multiplier:0.179;--x-height-multiplier:0.35;line-height:var(--lineHeight-relaxed);margin:var(--spacing-0) var(--spacing-0) var(--spacing-8) var(--spacing-0)}ol,p,ul{padding:var(--spacing-0)}ol,ul{list-style-image:none;list-style-position:outside;margin-bottom:var(--spacing-8);margin-left:var(--spacing-4);margin-right:var(--spacing-0)}ol li,ul li{padding-left:var(--spacing-0)}li>p,ol li,ul li{margin-bottom:calc(var(--spacing-8)/2)}li :last-child{margin-bottom:var(--spacing-0)}li>ul{margin-left:var(--spacing-8);margin-top:calc(var(--spacing-8)/2)}blockquote{border-left:var(--spacing-1) solid var(--color-key);color:var(--color-text-light);font-size:var(--fontSize-2);font-style:italic;margin-bottom:var(--spacing-8);margin-left:calc(var(--spacing-6)*-1);margin-right:var(--spacing-8);padding:var(--spacing-0) var(--spacing-0) var(--spacing-0) var(--spacing-6)}blockquote>:last-child{margin-bottom:var(--spacing-0)}blockquote>ol,blockquote>ul{list-style-position:inside}table{border-collapse:collapse;border-spacing:.25rem;margin-bottom:var(--spacing-8);width:100%}table thead tr th{border-bottom:1px solid var(--color-accent)}figcaption{font-size:.9em}a{color:var(--color-primary)}a:focus,a:hover{text-decoration:none}.global-wrapper{margin:var(--spacing-0) auto;max-width:var(--maxWidth-wrapper);padding:var(--spacing-10) var(--spacing-5)}.global-wrapper[data-is-root-path=true] .bio{margin-bottom:var(--spacing-20)}.global-header{display:flex;justify-content:space-between;margin-bottom:var(--spacing-12)}.header--menu{display:flex;margin-bottom:0}.header--menu li{color:var(--color-text);font-size:var(--fontSize-2);font-weight:900;letter-spacing:.18rem;line-height:var(--base-margin);list-style:none;margin-left:2rem;text-transform:uppercase}.header--menu a{color:var(--color-text);text-decoration:none}.header--menu a.active{color:var(--color-key)}.main-heading{font-size:var(--fontSize-7);margin:0}.post-list-item{margin-bottom:var(--spacing-8);margin-top:var(--spacing-8)}.post-list-item p{margin-bottom:var(--spacing-0)}.post-list-item h2{color:var(--color-key);font-size:var(--fontSize-4);margin-bottom:var(--spacing-2);margin-top:var(--spacing-0)}.post-list-item header{margin-bottom:var(--spacing-4)}.header-link-home{font-family:var(--font-heading);font-size:var(--fontSize-2);font-weight:var(--fontWeight-bold);text-decoration:none}.bio{display:flex;margin-bottom:var(--spacing-16)}.bio p,.bio-avatar{margin-bottom:var(--spacing-0)}.bio-avatar{border-radius:100%;margin-right:var(--spacing-4);min-width:50px}.blog-post header h1{margin:var(--spacing-0) var(--spacing-0) var(--spacing-4) var(--spacing-0)}.blog-post header p{font-family:var(--font-heading);font-size:var(--fontSize-2)}.blog-post-nav ul{margin:var(--spacing-0)}.gatsby-highlight{margin-bottom:var(--spacing-8)}.header--logo{align-items:center;display:flex;font-size:var(--fontSize-2);font-weight:var(--fontWeight-black);justify-items:center;text-decoration:none}.header--logo--text{color:var(--color-key);letter-spacing:.18rem;line-height:var(--base-margin);margin-left:var(--spacing-2);text-transform:uppercase}.svg_logo{height:var(--base-margin)}.svg_logo_path{fill:var(--color-key)}.copyright{font-size:12px;font-weight:200;letter-spacing:.2em;text-align:center;text-transform:uppercase}.bio{align-items:center}.blog-post img{max-width:100%}.blog-post table td,.blog-post table th{border:1px solid #777;padding:.5em}.responsive-iframe{overflow:hidden;padding-top:56.25%;position:relative;width:100%}.responsive-iframe iframe{bottom:0;height:100%;left:0;position:absolute;right:0;top:0;width:100%}@media(max-width:42rem){:root{--base-margin:16px}blockquote{margin-left:var(--spacing-0);padding:var(--spacing-0) var(--spacing-0) var(--spacing-0) var(--spacing-4)}ol,ul{list-style-position:inside}}@media(prefers-color-scheme:dark){:root{--color-base:#1e1c22;--color-text:#eee;--color-key:#1289ff;--color-text-light:#a4a4a4}}code[class*=language-],pre[class*=language-]{word-wrap:normal;background:none;color:#000;font-family:Consolas,Monaco,Andale Mono,Ubuntu Mono,monospace;font-size:1em;-webkit-hyphens:none;hyphens:none;line-height:1.5;tab-size:4;text-align:left;text-shadow:0 1px #fff;white-space:pre;word-break:normal;word-spacing:normal}code[class*=language-] ::selection,code[class*=language-]::selection,pre[class*=language-] ::selection,pre[class*=language-]::selection{background:#b3d4fc;text-shadow:none}@media print{code[class*=language-],pre[class*=language-]{text-shadow:none}}pre[class*=language-]{margin:.5em 0;overflow:auto;padding:1em}:not(pre)>code[class*=language-],pre[class*=language-]{background:#f5f2f0}:not(pre)>code[class*=language-]{border-radius:.3em;padding:.1em;white-space:normal}.token.cdata,.token.comment,.token.doctype,.token.prolog{color:#708090}.token.punctuation{color:#999}.token.namespace{opacity:.7}.token.boolean,.token.constant,.token.deleted,.token.number,.token.property,.token.symbol,.token.tag{color:#905}.token.attr-name,.token.builtin,.token.char,.token.inserted,.token.selector,.token.string{color:#690}.language-css .token.string,.style .token.string,.token.entity,.token.operator,.token.url{background:hsla(0,0%,100%,.5);color:#9a6e3a}.token.atrule,.token.attr-value,.token.keyword{color:#07a}.token.class-name,.token.function{color:#dd4a68}.token.important,.token.regex,.token.variable{color:#e90}.token.bold,.token.important{font-weight:700}.token.italic{font-style:italic}.token.entity{cursor:help}</style><title data-gatsby-head="true">인공지능의 역사에서 BERT 이해하기 | MediaNavi 미디어나비 Blog</title><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-144HB93NR5"></script><script>
    window.GATSBY_GTAG_PLUGIN_GA_TRACKING_ID = (
      'G-144HB93NR5'
    );
    window.GATSBY_GTAG_PLUGIN_ANONYMIZE = false;

    var options = {
      send_page_view: false
    };
    if (false) {
      options.anonymize_ip = true;
    }

    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    window.gtag = gtag;
    gtag('js', new Date());
    gtag('config', 'G-144HB93NR5', options);
  </script><style>.gatsby-image-wrapper{position:relative;overflow:hidden}.gatsby-image-wrapper picture.object-fit-polyfill{position:static!important}.gatsby-image-wrapper img{bottom:0;height:100%;left:0;margin:0;max-width:none;padding:0;position:absolute;right:0;top:0;width:100%;object-fit:cover}.gatsby-image-wrapper [data-main-image]{opacity:0;transform:translateZ(0);transition:opacity .25s linear;will-change:opacity}.gatsby-image-wrapper-constrained{display:inline-block;vertical-align:top}</style><noscript><style>.gatsby-image-wrapper noscript [data-main-image]{opacity:1!important}.gatsby-image-wrapper [data-placeholder-image]{opacity:0!important}</style></noscript><script type="module">const e="undefined"!=typeof HTMLImageElement&&"loading"in HTMLImageElement.prototype;e&&document.body.addEventListener("load",(function(e){const t=e.target;if(void 0===t.dataset.mainImage)return;if(void 0===t.dataset.gatsbyImageSsr)return;let a=null,n=t;for(;null===a&&n;)void 0!==n.parentNode.dataset.gatsbyImageWrapper&&(a=n.parentNode),n=n.parentNode;const o=a.querySelector("[data-placeholder-image]"),r=new Image;r.src=t.currentSrc,r.decode().catch((()=>{})).then((()=>{t.style.opacity=1,o&&(o.style.opacity=0,o.style.transition="opacity 500ms linear")}))}),!0);</script><link rel="alternate" type="application/rss+xml" title="MediaNavi blog" href="/rss.xml"/><link rel="icon" href="/favicon-32x32.png?v=6bfe33411e9e143f9e2c946e9de05911" type="image/png"/><link rel="manifest" href="/manifest.webmanifest" crossorigin="anonymous"/><link rel="apple-touch-icon" sizes="48x48" href="/icons/icon-48x48.png?v=6bfe33411e9e143f9e2c946e9de05911"/><link rel="apple-touch-icon" sizes="72x72" href="/icons/icon-72x72.png?v=6bfe33411e9e143f9e2c946e9de05911"/><link rel="apple-touch-icon" sizes="96x96" href="/icons/icon-96x96.png?v=6bfe33411e9e143f9e2c946e9de05911"/><link rel="apple-touch-icon" sizes="144x144" href="/icons/icon-144x144.png?v=6bfe33411e9e143f9e2c946e9de05911"/><link rel="apple-touch-icon" sizes="192x192" href="/icons/icon-192x192.png?v=6bfe33411e9e143f9e2c946e9de05911"/><link rel="apple-touch-icon" sizes="256x256" href="/icons/icon-256x256.png?v=6bfe33411e9e143f9e2c946e9de05911"/><link rel="apple-touch-icon" sizes="384x384" href="/icons/icon-384x384.png?v=6bfe33411e9e143f9e2c946e9de05911"/><link rel="apple-touch-icon" sizes="512x512" href="/icons/icon-512x512.png?v=6bfe33411e9e143f9e2c946e9de05911"/></head><body><div id="___gatsby"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><div class="global-wrapper" data-is-root-path="false"><header class="global-header"><a class="header--logo" href="/"><svg xmlns="http://www.w3.org/2000/svg" role="img" class="svg_logo" preserveAspectRatio="none" viewBox="0 0 205.71 90"><title>미디어나비 로고</title><path class="svg_logo_path" d="M202.22,0H176.37a9,9,0,0,0-7.52,5.12l-19,43.4-19-43.4A9,9,0,0,0,123.35,0H97.4a9,9,0,0,0-7.52,5.12L79.15,29.64,75.29,5.12A6.32,6.32,0,0,0,69.2,0H42.88a9,9,0,0,0-7.52,5.12L.46,84.88C-.78,87.71.58,90,3.5,90H29.34a9,9,0,0,0,7.52-5.12L47.59,60.36l3.86,24.52A6.33,6.33,0,0,0,57.54,90H83.86a9,9,0,0,0,7.53-5.12l19-43.4,19,43.4A9,9,0,0,0,136.89,90h25.94a9,9,0,0,0,7.52-5.12L205.26,5.12C206.49,2.29,205.13,0,202.22,0ZM51.51,51.4,70.44,8.16,75.23,38.6,56.31,81.84Zm61.73-16.46L125.66,6.56a2.8,2.8,0,0,1,.36.56l21,47.94L134.58,83.44a2.42,2.42,0,0,1-.36-.56Z"></path></svg><span class="header--logo--text">Blog</span></a><ul class="header--menu"><li><a href="/">Posts</a></li><li><a href="/works">Works</a></li></ul></header><main><article class="blog-post" itemscope="" itemType="http://schema.org/Article"><header><h1 itemProp="headline">인공지능의 역사에서 BERT 이해하기</h1><p>2022.01.21.</p></header><section itemProp="articleBody"><div class="gatsby-resp-iframe-wrapper" style="padding-bottom: 56.25%; position: relative; height: 0; overflow: hidden; margin-bottom: 1.0725rem" > <div class="responsive-iframe"> <iframe src="https://www.youtube-nocookie.com/embed/Pj6563CAnKs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" style=" position: absolute; top: 0; left: 0; width: 100%; height: 100%; "></iframe> </div> </div>
<h2>0. 들어가기 앞서 - 인공지능의 발전사</h2>
<p><span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; "
    >
      <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 64.55696202531645%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAABAACBf/EABUBAQEAAAAAAAAAAAAAAAAAAAIB/9oADAMBAAIQAxAAAAHjuCpgdmi//8QAGRABAAMBAQAAAAAAAAAAAAAAAQIDESEA/9oACAEBAAEFAonKaz0jJDyNri6//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAGBABAQADAAAAAAAAAAAAAAAAARAAEYH/2gAIAQEABj8Cx2HaE//EABkQAAMBAQEAAAAAAAAAAAAAAAABIRExQf/aAAgBAQABPyFOGzpjVPBuJjZkJcoWGf/aAAwDAQACAAMAAAAQyA//xAAWEQEBAQAAAAAAAAAAAAAAAAARAAH/2gAIAQMBAT8Q1bG//8QAFhEBAQEAAAAAAAAAAAAAAAAAEQAB/9oACAECAQE/EMC0v//EABsQAQADAAMBAAAAAAAAAAAAAAEAESExQVHh/9oACAEBAAE/EKQXfXEXZNBDAcp82OSpPIMCgHa1uGLQlX2Ogaz/2Q=='); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="인간형 로봇이 복잡한 수식을 보고 고민하는 모습"
        title="인간형 로봇이 복잡한 수식을 보고 고민하는 모습"
        src="/static/bc1536de6d1a08a7a335d48231cbb216/828fb/image01.jpg"
        srcset="/static/bc1536de6d1a08a7a335d48231cbb216/ff44c/image01.jpg 158w,
/static/bc1536de6d1a08a7a335d48231cbb216/a6688/image01.jpg 315w,
/static/bc1536de6d1a08a7a335d48231cbb216/828fb/image01.jpg 630w,
/static/bc1536de6d1a08a7a335d48231cbb216/0ede0/image01.jpg 945w,
/static/bc1536de6d1a08a7a335d48231cbb216/e5166/image01.jpg 1200w"
        sizes="(max-width: 630px) 100vw, 630px"
        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"
        loading="lazy"
        decoding="async"
      />
    </span>
이미지출처 : <a href="https://post.naver.com/viewer/postView.nhn?volumeNo=27021892&#x26;memberNo=43011790">https://post.naver.com/viewer/postView.nhn?volumeNo=27021892&#x26;memberNo=43011790</a></p>
<ul>
<li>1950년대 영국수학자 앨런 튜링의 계산기계와 지능이라는 논문으로 시작
<ul>
<li>존 폰 노이만이 현대 컴퓨터 구조의 표준을 고안</li>
<li>on/off 의 기초기능 단위를 그물망 형태로 연결하여 인간의 뇌를 흉내 => 인공신경망</li>
<li>신경내에서의 반복적인 신호를 신경세포가 학습함을 확인 => 퍼셉트론(Perceptron:인공신경뉴런)의 탄생(1958) 신경망기반 인공지능의 1차부흥기</li>
</ul>
</li>
<li>1969년
<ul>
<li>퍼셉트론의 XOR처리 불가능을 수학적으로 증명하며 인공신경망의 암흑기</li>
</ul>
</li>
<li>1970년대이후
<ul>
<li>통계기술로 중심이동, 데이터마이닝을 거쳐 빅데이터의 근간이 됨</li>
</ul>
</li>
<li>1980년대
<ul>
<li>0/1사이의 여러값을 가질 수 있는  퍼지이론에 기반한 인공지능->전문가시스템</li>
</ul>
</li>
<li>1990년대
<ul>
<li>슈퍼컴퓨터와 시뮬레이션으로 연구방향 전환</li>
<li>강화학습, 역전파기법 등이 발표되지만, 컴퓨팅 성능의 한계로 주목받지 못함</li>
</ul>
</li>
<li>2000년대
<ul>
<li>심층신경망(딥러닝) 기술의 발전</li>
</ul>
</li>
<li>2010년대
<ul>
<li>Deep-CNN(합성곱신경망)의 이미지 인식 성능의 비약적 발전으로 딥러닝이 주목</li>
</ul>
</li>
<li>2014년
<ul>
<li>구글의 딥마인드(영국의 강화학습 회사) 인수, 알파고로 바둑에서 인간을 능가 => ‘인공지능’ 이라는 용어의 대중화</li>
</ul>
</li>
<li>현재
<ul>
<li>CNN(이미지인식), RNN(음성/문자인식), GAN(모델간 대립으로 성능개선), 추론(Reasoning), 전이학습(Transfer learning)</li>
</ul>
</li>
</ul>
<h2>1. 자연어 처리 (NLP) 란</h2>
<p><span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 580px; "
    >
      <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 52.53164556962025%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAEDAv/EABYBAQEBAAAAAAAAAAAAAAAAAAEDBP/aAAwDAQACEAMQAAABdMVtkBif/8QAGhAAAgIDAAAAAAAAAAAAAAAAARICEQAQE//aAAgBAQABBQLoXlKjih6B1//EABURAQEAAAAAAAAAAAAAAAAAABAR/9oACAEDAQE/Aaf/xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAZEAACAwEAAAAAAAAAAAAAAAAAEQEQIVH/2gAIAQEABj8CSxoiO0za/8QAGhAAAgMBAQAAAAAAAAAAAAAAAREAITEQcf/aAAgBAQABPyFXoExB8MMhYyOBAVkIVCf/2gAMAwEAAgADAAAAEEsP/8QAGBEAAwEBAAAAAAAAAAAAAAAAAAERIUH/2gAIAQMBAT8QcO8LdR//xAAVEQEBAAAAAAAAAAAAAAAAAAAQEf/aAAgBAgEBPxCH/8QAGxABAAMBAQEBAAAAAAAAAAAAAQARITFBUWH/2gAIAQEAAT8QE2a6D3yKXtQz4Xn7C3uMLdDLK5FIp2TkBUUE/9k='); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="Natural Language Processing (NLP) For Artificial Intelligence)"
        title="Natural Language Processing (NLP) For Artificial Intelligence)"
        src="/static/1f8ec497a23b748f1582820176f7de49/78d0c/image02.jpg"
        srcset="/static/1f8ec497a23b748f1582820176f7de49/ff44c/image02.jpg 158w,
/static/1f8ec497a23b748f1582820176f7de49/a6688/image02.jpg 315w,
/static/1f8ec497a23b748f1582820176f7de49/78d0c/image02.jpg 580w"
        sizes="(max-width: 580px) 100vw, 580px"
        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"
        loading="lazy"
        decoding="async"
      />
    </span>
이미지출처 : <a href="https://thinkpalm.com/blogs/natural-language-processing-nlp-artificial-intelligence/">https://thinkpalm.com/blogs/natural-language-processing-nlp-artificial-intelligence/</a></p>
<p>자연어 처리(Natural Language Processing)를 말하기 위해서 먼저 자연어란 무엇인가.</p>
<p>자연어는 인간이 쓰고있는 언어 자체를 말한다. 인간은 생각하는 동물이고, 그 생각은 다른 인간에게 자동으로 전달되지 못한다. 따라서 그 생각의 전달을 위해 오랜기간 적용되고 발전된 효율적인 방법으로 음성과 문자를 이용한 언어라는 방법이 생성되었다. 같은 사회에 사는 인간들은 그 방식을 자연스럽게 습득하여 소통하며 생활하는 것이다.</p>
<p>우리는 아직 인간의 머리속의 작동방식을 잘 모른다. 하지만 언어(자연어)를 통해 내 머리속의 생각을 다른 사람의 머리속에 이해시킬 수 있다.
우리는 컴퓨터의 작동방식은 잘 안다. 우리가 만들었으니까. 따라서 우리는 컴퓨터가 사람의 생각을 이해할 수 있는 명확한 입력방식을 안다. 이 소통방법도 ‘언어’라고 표현한다. 많은 프로그래밍 언어들, 예로 어셈블리, C, 자바, 파이썬 등등이 컴퓨터를 이해시키는 언어들이다.
우리는 이러한 언어를 별도로 ‘프로그래밍 언어’라고 부른다.</p>
<p>자연스러운 인간의 언어에는 단어 자체의 의미 외에도 문맥적, 함축적, 반어적, 비유적 등의 각종 의미와 표현들이 순서도치, 축약, 생략, 장황하고 완곡한 표현 등의 방법으로 적용되기에 기계가 이해하기에 너무 많은 변수들이 존재한다.
이러한 인간의 언어인 자연어를 기계에게 이해시키기 위한 방법들을 일컬어 자연어 처리, NLP라고 부르고 있다.</p>
<p>인간이 컴퓨터에게 무엇인가의 결과물을 얻기위한 입력방식으로 모든 인간이 자연스럽게 체득하고 있는 이 자연어로 입력할 수 있다면 활용방안이 무궁무진해 질 것이다.</p>
<h2>2. 인공지능(AI:Artificial Intelligence)의 발전</h2>
<p>NLP의 발전은 인공지능의 발전과 맞물려 있다. 인공지능의 목표가 인간과 같은 지능을 가진 기계를 만드는 것이고, 그것을 인간이 평가할 수 있는 방법은 언어를 통한 소통이 될 수 밖에 없으니까.</p>
<p>여기서 말하는 인간과 같은 인공지능은 강인공지능 이라는 용어로 표현된다. 영어로는 범용인공지능 정도로 표현되는 AGI(Artficial General Intelligence)라고 부른다. 하지만 실제로 구현하기에는 너무나 막막하고 기계의 성능도 초보단계이다 보니 강인공지능으로 가기위해 각각의 세부 테스크별로 개발해 나가는 방향으로 발전하게 된다. 이미지에서 사물인식이라던지, 음성에서 텍스트를 뽑아낸다던지, 텍스트에서 카테고리를 분류한다던지 하는 식으로 특정 테스크에 집중하여 구현하려 하였다.</p>
<h3>명시적 알고리즘 구현방식의 한계</h3>
<p>1940년대 2차대전 당시 적군의 암호해독을 위한 기계번역 기법을 시작으로 많은 기법들이 나오고 발전하고 있다. 하지만, 명시적 알고리즘이라는 것이 모두 사람이 데이터를 보고 고민해서 처리로직을 추가하는 형태로써 그 법칙을 다 알아낼 수도 없을 뿐더러, 아무리 많은 로직을 쓴다 한들 복잡해진 구조에서 오는 예상못한 사이드이펙트 등으로 성능발전은 더딜 수 밖에 없었다. 인간의 처리 프로세스를 모르면서 세부적인 로직을 구현하려 한 것이니 당연한 결과였을 것이다.</p>
<h3>머신러닝의 등장</h3>
<p>1990년대 이후의 컴퓨팅 성능 발달과 빅데이터의 활용으로 새로운 국면으로 접어든다.</p>
<p>프로그래머가 자연어 이해의 세부로직을 구상하고 구현하는 방식이 아닌 대량의 데이터를 기계가 학습하여 그 통계적 유사성을 바탕으로 머신이 직접 알고리즘을 학습하는 머신러닝의 방법이 고안되고 적용되었다.</p>
<h3>컴퓨팅발전으로 머신러닝의 방식에 딥러닝 기법이 도입</h3>
<p>2000년대 컴퓨팅 능력의 발전과 더불어 GPU 등 병렬연산 기법의 도입으로 NLP에서 비약적 발전을 이룬다. 언어를 수치적 방향성을 가지는 벡터로 표현함으로써 단어의 유사성을 비교할 수 있게 됨과 더불어, 행렬식 표현으로 병렬연산이 가능한 형태로 이는 게임산업에 의해 발달한 병렬연산GPU를 자연어 처리 연산에 활용할 수 있게 됨으로써 많은 성능 향상을 가져왔다.</p>
<p>인간의 신경세포의 내부적인 기작은 정확히 모르지만 충분한 신경망을 거쳐 결과값을 찾아내는 이른바 딥러닝 기법의 적용으로 각 테스크별로는 인간 수준의 해답을 내 놓는 수준에 이른다.</p>
<p>다층 퍼셉트론이란 입력층과 출력층 사이에 은닉층을 추가하면, 이 은닉층에서 우리가 찾기 어려운 데이터의 특징들을 수치로 포착하여 더 복잡한 문제를 풀 수 있다는 개념으로 실제 신경망이 여러층을 거쳐 지나가는 구조를 모방하였다.</p>
<h3>머신러닝의 성능향상과 데이터</h3>
<p>어떤 통계적인 공통점을 특징으로 찾아내려면 많은 데이터가 유리하다는 것은 당연하다.</p>
<p>많은 데이터에 더하여 정확한 데이터 또한 중요한 요소로 적용된다. 부정확한 데이터가 많다면 특징을 찾아내는데 방해요소로 적용하게 될 것이고, 이는 성능저하로 이어지기 때문이다.</p>
<p>빅데이터 시대를 거치며 대량의 데이터에 의한 머신러닝의 성능향상이 가능해졌다. 하지만 많은 데이터를 만드는데도 많은 리소스가 소요되고, 많은 데이터가 있으면 그만큼 학습 비용도 상승하게 된다.</p>
<h2>3. 언어 모델의 발전</h2>
<p>NLP에서 단어의 다음에 올 단어를 예측하여 문장을 완성하는 모델을 언어 모델이라고 한다.</p>
<h3>3.1. 합성곱신경망 CNN(Convolutional Neural Network)</h3>
<p><span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; "
    >
      <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 37.9746835443038%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAIABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAQD/8QAFQEBAQAAAAAAAAAAAAAAAAAAAAH/2gAMAwEAAhADEAAAAb6CNRX/xAAXEAEBAQEAAAAAAAAAAAAAAAABABET/9oACAEBAAEFAjuJtt//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAbEAABBAMAAAAAAAAAAAAAAAARAAECECEyQf/aAAgBAQAGPwKJy3VqK//EABoQAAIDAQEAAAAAAAAAAAAAAAABESExQVH/2gAIAQEAAT8hyOy8DvqtXZE3h//aAAwDAQACAAMAAAAQ/A//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAbEAEBAAIDAQAAAAAAAAAAAAABEQAxIUGRYf/aAAgBAQABPxAgkgiqWsL5rDe9KAWc+axZaPkuf//Z'); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="How to build CNN model for Dog Breed Classifier with TensorFlow)"
        title="How to build CNN model for Dog Breed Classifier with TensorFlow)"
        src="/static/6fb5468b487d5233dbfff784889f4275/828fb/image03.jpg"
        srcset="/static/6fb5468b487d5233dbfff784889f4275/ff44c/image03.jpg 158w,
/static/6fb5468b487d5233dbfff784889f4275/a6688/image03.jpg 315w,
/static/6fb5468b487d5233dbfff784889f4275/828fb/image03.jpg 630w,
/static/6fb5468b487d5233dbfff784889f4275/faddd/image03.jpg 850w"
        sizes="(max-width: 630px) 100vw, 630px"
        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"
        loading="lazy"
        decoding="async"
      />
    </span>
이미지출처 : <a href="https://medium.com/@ajay.pratap/how-to-build-dog-breed-classifier-with-cnn-model-using-tensorflow-1c4759fcf601">https://medium.com/@ajay.pratap/how-to-build-dog-breed-classifier-with-cnn-model-using-tensorflow-1c4759fcf601</a></p>
<ul>
<li>이미지 처리 분야에서 강력한 모델인 합성곱처리방식을 NLP분야에 적용하였다.</li>
<li>문장을 벡터의 행렬형태로 표현하여 합성곱으로 연산하는 방법이다.</li>
<li>이미지 처리에서 CNN은 Filter라는 특정영역의 합성곱 연산을 통하여 하나의 값으로 표현하는 방식을 전체 행렬에 순차적으로 이동하며 구한 새로운 행렬에서 원하는 특징을 도출해 내는 것으로, 각 이미지의 경계선 등 특징을 구별해 낼 수 있다.</li>
<li>내부적으로는 convolution과 pooling 레이어의 반복에 의해 특징을 일렬로 늘어뜨린다음 분류기를 통해 원하는 결과값을 출력하도록 만든다.</li>
<li>이러한 이미지에서의 강점을 NLP에서도 기대하며 적용해 보니 제법 괜찮은 성능을 보였다.</li>
<li>아래 사진은 각 레이어 마다 적용되는 방법을 시각화 한 예시이다.</li>
</ul>
<p><span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 464px; "
    >
      <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 107.59493670886076%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAWABQDASIAAhEBAxEB/8QAGQABAQADAQAAAAAAAAAAAAAAAAQBAgMF/8QAFgEBAQEAAAAAAAAAAAAAAAAAAgEA/9oADAMBAAIQAxAAAAGamzAPlOQzWkW5CX//xAAdEAACAgEFAAAAAAAAAAAAAAABAgADEhETISIy/9oACAEBAAEFAtZXVmreqgldJdcjzASJudp//8QAFREBAQAAAAAAAAAAAAAAAAAAASD/2gAIAQMBAT8BCP/EABURAQEAAAAAAAAAAAAAAAAAABIg/9oACAECAQE/ATH/xAAeEAACAgAHAAAAAAAAAAAAAAABEQAQAhIhMUFhcf/aAAgBAQAGPwKNqGZiuzBrw69mEgbBV//EABsQAAIDAQEBAAAAAAAAAAAAAAABESExUXFB/9oACAEBAAE/IdbZp9vq0ohYKDmlToPReHhJp7YgcOtIisZehuHR/9oADAMBAAIAAwAAABBYD0D/xAAYEQACAwAAAAAAAAAAAAAAAAAAARARQf/aAAgBAwEBPxDVlT//xAAYEQACAwAAAAAAAAAAAAAAAAAAARARQf/aAAgBAgEBPxBJpU//xAAfEAEAAgMAAQUAAAAAAAAAAAABABEhUWExQXGRscH/2gAIAQEAAT8QaERbyHowLiRGVBs+pTAQ1T5xGyWmucrUVWUdFprv5LW9WflBarBRYnSZaVQWsLE9sy0AV2f/2Q=='); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="원본 사진: 잔디 위에 앉은 개 사진"
        title="원본 사진: 잔디 위에 앉은 개 사진"
        src="/static/7527d2539fc314b0c7f0af018ddebbf8/c9a08/image04.jpg"
        srcset="/static/7527d2539fc314b0c7f0af018ddebbf8/ff44c/image04.jpg 158w,
/static/7527d2539fc314b0c7f0af018ddebbf8/a6688/image04.jpg 315w,
/static/7527d2539fc314b0c7f0af018ddebbf8/c9a08/image04.jpg 464w"
        sizes="(max-width: 464px) 100vw, 464px"
        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"
        loading="lazy"
        decoding="async"
      />
    </span>
<span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; "
    >
      <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 30.37974683544304%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAIAAABM9SnKAAAACXBIWXMAAAsTAAALEwEAmpwYAAABeUlEQVQY0wFuAZH+AK/IzWCTom2dqG6qoW+eo26Wpm+bpG6SpnKKp3eZm2aUoGadq2aOrHaVoXifnmeUq2aZqm2HqniQlqO7tgCCqLARUnElcnYkYXYkVHghcnQdeHYhZXcviGcvhGsdZn4RaYAxZ21xoT87fGMXW38WZn9HdWFcnDxwmJEAw9fXjre0mMG5lLa0mLm2kba8kLW5j7G5lbe7j7K3iLG3jbu+l6i4n6Wtma+5kLm9j7q3m6+2lau0s8LPAMbb3J2/xqbGyqHWxqXJx6m9yKvDxai3yK2uy6Wzvp62v6fByqC1z6Czzam+yKW9y6S9yaOvzqaxx8bU1AB4nqYeQ3EsdngrX3QuSnUrenUpcHgpZXougXEyhHEsa3weaYIzanRrpUpChWUhV4IiY39EdmpYnkd3oZIAjrewSYh1UZ+BUYt+UYp+RIuJSYqGSYqGRZGGR5GFQ4qJQouLU3WBZYJzTYCFSIuGUph/XIN9TIF4e6Cr7/a/b6SFF78AAAAASUVORK5CYII='); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="How to build CNN model for Dog Breed Classifier with TensorFlow)"
        title="How to build CNN model for Dog Breed Classifier with TensorFlow)"
        src="/static/804956aae7ea785e857d470769921cc2/f058b/image05.png"
        srcset="/static/804956aae7ea785e857d470769921cc2/c26ae/image05.png 158w,
/static/804956aae7ea785e857d470769921cc2/6bdcf/image05.png 315w,
/static/804956aae7ea785e857d470769921cc2/f058b/image05.png 630w,
/static/804956aae7ea785e857d470769921cc2/40601/image05.png 945w,
/static/804956aae7ea785e857d470769921cc2/78612/image05.png 1260w,
/static/804956aae7ea785e857d470769921cc2/1d499/image05.png 1632w"
        sizes="(max-width: 630px) 100vw, 630px"
        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"
        loading="lazy"
        decoding="async"
      />
    </span>
<span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; "
    >
      <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 35.44303797468354%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAIAAACHqfpvAAAACXBIWXMAAAsTAAALEwEAmpwYAAABlUlEQVQY0wXBW2/SUAAA4P5RnzSLezLGRRPjwyAZjOEyCZOFLaNQWgrn9Jyu9zstbVdK23PKvC3DaWKm/8PvY/Svd8+E6R5AzxfoA75+BYQDkT/Cwya+2hfGLzj+JYD7wuKtyDW0qz0Bvpb4A5V7M7+ehCqT/HxsuvbZKmz7/mWgn0fuMDTYAI+WRn9lnLjWWeT1Y5cN8Xip9CJ/4FufPW3oa2aVMetfu0NdbVl+03R6nnLq6KeaceHhi0BuoJtDiI4tu6U7H3VtEMltxe045klgdE1FIxGzvH94p0pHrvZeVbo26CzlT868b0/PrUXHkxouPvbkloV71nTg8R1bbumobYGuId6sPeb+39OsLOVtDSmBlJrKCmzooqolejfPSwkEEiHatoK0BiXFWypRigiZFTTYPTI//j4JRYHqGpIKkFLVY7ghkBSIVnySicABZaHWJaIElhtIKkQJpuUsz8PdA/Plz+9hHHP5epKloywRF+44vmWzhMvTUZRMeHOUxOIm4fJbNo3ZdcrlGZ+nl/HK/P7tP7R4IssQUdQ4AAAAAElFTkSuQmCC'); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="How to build CNN model for Dog Breed Classifier with TensorFlow)"
        title="How to build CNN model for Dog Breed Classifier with TensorFlow)"
        src="/static/a09645cf4085584c2644f12ce795af30/f058b/image06.png"
        srcset="/static/a09645cf4085584c2644f12ce795af30/c26ae/image06.png 158w,
/static/a09645cf4085584c2644f12ce795af30/6bdcf/image06.png 315w,
/static/a09645cf4085584c2644f12ce795af30/f058b/image06.png 630w,
/static/a09645cf4085584c2644f12ce795af30/40601/image06.png 945w,
/static/a09645cf4085584c2644f12ce795af30/78612/image06.png 1260w,
/static/a09645cf4085584c2644f12ce795af30/b9460/image06.png 1641w"
        sizes="(max-width: 630px) 100vw, 630px"
        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"
        loading="lazy"
        decoding="async"
      />
    </span>
이미지출처 : <a href="https://www.analyticsvidhya.com/blog/2020/11/tutorial-how-to-visualize-feature-maps-directly-from-cnn-layers/">https://www.analyticsvidhya.com/blog/2020/11/tutorial-how-to-visualize-feature-maps-directly-from-cnn-layers/</a></p>
<h3>3.2. 순환신경망 RNN(Recurrent Neural Network)</h3>
<p>입력층에서 출력층으로만 향하는 기존의 방식들은 문장에서 앞의 단어가 영향을 미치는 구조일 경우 반영이 되지 않는다.</p>
<p><span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; "
    >
      <a
    class="gatsby-resp-image-link"
    href="/static/e1814b6aa43aa0f90817c2edb6c82354/69476/image07.png"
    style="display: block"
    target="_blank"
    rel="noopener"
  >
    <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 46.835443037974684%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAIAAAC9o5sfAAAACXBIWXMAAAsTAAALEwEAmpwYAAABVklEQVQoz5WQXW+jMBBF+f9/adVVVbWq2t3tR5REiqEUMGgTEwJkbLCN7VuRpFKfKvU+ztEZzdwIn9Fad31PxyMR9X2vB2m1xreJAuCcDz4opUrO21PEbjcv2IvJWO88ADc57/1FCpgmN8usUZt1+fZShGBl154ptXJ5l1gt02UWP2cAlvdM8OZMBzk+/V7NMkn5nqbr1XovdpO14YSllEXBR+qauq7KCkCRF0TyIg8qz/MARELUT+zub3Kd5ZnRRkoZPPg+eUyucv72mtz/S26q/+XD5opVi7bt2kO3yRYP7FcnmwhAe2yyMmVs07afZ6s+LWIATVdnZQrgncc9faXMBRdV/bgqxCIugeDcpZKO1O1rAiCt9i+MA/izzvnuLAc16pvn+PTzMHZEPZE1xlp7lr33gzYAzDSNZh6OxtpTwzMNQWkTECKiYy3E4dBst1siwk/yARyV/xF6h67iAAAAAElFTkSuQmCC'); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="RNN"
        title="RNN"
        src="/static/e1814b6aa43aa0f90817c2edb6c82354/f058b/image07.png"
        srcset="/static/e1814b6aa43aa0f90817c2edb6c82354/c26ae/image07.png 158w,
/static/e1814b6aa43aa0f90817c2edb6c82354/6bdcf/image07.png 315w,
/static/e1814b6aa43aa0f90817c2edb6c82354/f058b/image07.png 630w,
/static/e1814b6aa43aa0f90817c2edb6c82354/69476/image07.png 926w"
        sizes="(max-width: 630px) 100vw, 630px"
        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"
        loading="lazy"
        decoding="async"
      />
  </a>
    </span></p>
<p>이를 보완하기 위해 결과값을 출력층으로만 보내지 않고 다음 단어의 입력층에도 전달하여 다음 단어가 이전 단어의 정보를 같이 판단할 수 있게 만드는 구조들이 고안되었는데, 그중 가장 유명해진 방법이 이 RNN이다.</p>
<p>RNN은 최초 입력부터 각 입력마다의 결과인 Hidden state가 그 이전의 입력값을 기억하는 구조로 작동하기 때문이다.</p>
<p>이렇게 함으로써 이전 단어들의 특징을 전달할 수 있게 되었지만, 순차적 연산을 거쳐야 하고, 연산량을 줄이기 위해 직전의 가까운 단어정보만 전달을 받게 됨으로 여러 문제 또한 발생시킨다.</p>
<h3>3.3. seq2seq 언어모델</h3>
<ul>
<li>언어모델이란 문장을 차례로 생성해 내는 모델이다.</li>
<li>Machine Translation(기계번역)은 과거 의미구문별 1:1매핑 후 문법적 재배열 하는 phrase-based MT 였으나 neural MT로 넘어오면서 비약적인 발전을 한다.</li>
<li>그 시작을 알린 기술이 sequence to sequence 언어모델이다.</li>
<li>번역될 문장과 번역된 문장을 조건부 확률의 문제로 해석한다.</li>
<li>또한 암호를 해석하는 인코더-디코더 개념을 도입한 모델이다.</li>
<li>seq2seq는 RNN을 이용한 인코더와 디코더의 두부분의 조합으로 구성된다.</li>
</ul>
<p><span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; "
    >
      <a
    class="gatsby-resp-image-link"
    href="/static/f6bc1f50dbd7213a61a376ac4116fdb7/6a068/image08.jpg"
    style="display: block"
    target="_blank"
    rel="noopener"
  >
    <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 56.32911392405063%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAABAAC/8QAFQEBAQAAAAAAAAAAAAAAAAAAAQL/2gAMAwEAAhADEAAAAX5fSlmVn//EABoQAQACAwEAAAAAAAAAAAAAAAEAAgMSExH/2gAIAQEAAQUC7I97SmVTUmtZ4T//xAAVEQEBAAAAAAAAAAAAAAAAAAABEP/aAAgBAwEBPwFZ/8QAFhEBAQEAAAAAAAAAAAAAAAAAAQIQ/9oACAECAQE/AZEz/8QAGBAAAwEBAAAAAAAAAAAAAAAAAAExEEH/2gAIAQEABj8ChzIiIh//xAAbEAEBAAIDAQAAAAAAAAAAAAABABExQVHBgf/aAAgBAQABPyFUDlPTHihucvlZjyg9A+X/2gAMAwEAAgADAAAAEPcP/8QAFhEBAQEAAAAAAAAAAAAAAAAAARAR/9oACAEDAQE/EATCf//EABYRAQEBAAAAAAAAAAAAAAAAAAEAEf/aAAgBAgEBPxBDVtL/xAAbEAEAAgMBAQAAAAAAAAAAAAABABEhMVFBwf/aAAgBAQABPxA/1DNL9l7AWGxicrxfCLsoqrhAC2x6IbR3AE//2Q=='); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="RNN"
        title="RNN"
        src="/static/f6bc1f50dbd7213a61a376ac4116fdb7/828fb/image08.jpg"
        srcset="/static/f6bc1f50dbd7213a61a376ac4116fdb7/ff44c/image08.jpg 158w,
/static/f6bc1f50dbd7213a61a376ac4116fdb7/a6688/image08.jpg 315w,
/static/f6bc1f50dbd7213a61a376ac4116fdb7/828fb/image08.jpg 630w,
/static/f6bc1f50dbd7213a61a376ac4116fdb7/0ede0/image08.jpg 945w,
/static/f6bc1f50dbd7213a61a376ac4116fdb7/6a068/image08.jpg 960w"
        sizes="(max-width: 630px) 100vw, 630px"
        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"
        loading="lazy"
        decoding="async"
      />
  </a>
    </span></p>
<p>인코더에 번역 될 문장이 RNN구조로 입력되고, 디코더에서는 타겟언어의 해당문장이 RNN을 통해서 단어별로 생성되는 생성모델 구조이다. 번역 같은 좌우 대칭구조에 적합하고 좋은 성능을 보여준다.</p>
<p>RNN의 메모리특성인 hidden state중 입력단의 마지막 hidden state를 출력단이 계속 참고하여 다음 단어를 예측하는 구조로 작동한다. 하지만 RNN은 이전 입력을 기억하긴 하지만, 중심단어와 연관단어의 거리가 너무 멀어지면 성능이 저하되는 현상이 있다. 이는 인코더 입력단의 누적 hidden state가 최종적으로 문장벡터라고 불리는 하나의 벡터에 기록되기 때문에 발생하는 현상이다. 이 현상을 줄이려고 모델 크기를 마냥 크게하는 것은 새롭게 효율성의 문제를 야기하게 된다. 이 문제를 해결하기 위해 아래의 Attention 메카니즘이 탄생한다.</p>
<h3>3.4. Attention 메커니즘</h3>
<p><span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; "
    >
      <a
    class="gatsby-resp-image-link"
    href="/static/eabe9a7691eb1880a185a20ca61b42bc/6a068/image09.jpg"
    style="display: block"
    target="_blank"
    rel="noopener"
  >
    <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 56.32911392405063%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAABAAC/8QAFQEBAQAAAAAAAAAAAAAAAAAAAQL/2gAMAwEAAhADEAAAAX5fSlmVn//EABoQAAIDAQEAAAAAAAAAAAAAAAABAhESEwP/2gAIAQEAAQUC7U+8iHq2sozEpH//xAAVEQEBAAAAAAAAAAAAAAAAAAABEP/aAAgBAwEBPwFZ/8QAFhEBAQEAAAAAAAAAAAAAAAAAARAR/9oACAECAQE/AQyf/8QAGRABAAIDAAAAAAAAAAAAAAAAAAExEDJB/9oACAEBAAY/AnMVDWFP/8QAGxABAAMAAwEAAAAAAAAAAAAAAQARMYGxwfH/2gAIAQEAAT8hUtVsfiiSw2NvlLg6IFgOJ//aAAwDAQACAAMAAAAQpw//xAAWEQEBAQAAAAAAAAAAAAAAAAARARD/2gAIAQMBAT8QSGf/xAAYEQADAQEAAAAAAAAAAAAAAAAAARGRwf/aAAgBAgEBPxC6tvbwqP/EAB4QAQACAgEFAAAAAAAAAAAAAAEAEUFRITFhccHR/9oACAEBAAE/ECUBEZfc4NdRt9j018tEXZRVVpC8LRkwehuwn//Z'); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="SEQ2SEQ + Attention"
        title="SEQ2SEQ + Attention"
        src="/static/eabe9a7691eb1880a185a20ca61b42bc/828fb/image09.jpg"
        srcset="/static/eabe9a7691eb1880a185a20ca61b42bc/ff44c/image09.jpg 158w,
/static/eabe9a7691eb1880a185a20ca61b42bc/a6688/image09.jpg 315w,
/static/eabe9a7691eb1880a185a20ca61b42bc/828fb/image09.jpg 630w,
/static/eabe9a7691eb1880a185a20ca61b42bc/0ede0/image09.jpg 945w,
/static/eabe9a7691eb1880a185a20ca61b42bc/6a068/image09.jpg 960w"
        sizes="(max-width: 630px) 100vw, 630px"
        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"
        loading="lazy"
        decoding="async"
      />
  </a>
    </span></p>
<p>어텐션 메커니즘은 seq2seq에서 인코더 부분의 RNN이 바로 직전의 가까운 단어만을 잘 기억하고 문장 초반의 거리가 멀어진 단어를 망각하는 특성에 의한 성능저하를 극복하고자 제시되었다.</p>
<p>기존의 직전 단어의 결과값을 누적으로 넘겨받아 하나의 벡터를 형성하는게 아니라 그 이전의 각 단어의 결과값(hidden state)을 한꺼번에 참고하여 디코더에 단어가 생성되는 순간에 중요도가 높은 인코더부분의 hidden state를 중요하게 참조하는 구조이다.</p>
<p>중요한 단어는 모델이 데이터를 학습하면서 스스로 연결관계를 알아내는 구조이다. 이러한 구조는 seq2seq에서 긴 문장에서도 매우 좋은 성능을 보여주었다.</p>
<p><img src="/11a43c372c778bf743457c31d1391092/image10.gif" alt="SEQ2SEQ + Attention 과정"></p>
<h3>3.5. Transformer</h3>
<p><span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; "
    >
      <a
    class="gatsby-resp-image-link"
    href="/static/65ac8ec7c076a56f7ce9113e423d0a74/6a068/image11.jpg"
    style="display: block"
    target="_blank"
    rel="noopener"
  >
    <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 56.32911392405063%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAQCBf/EABUBAQEAAAAAAAAAAAAAAAAAAAID/9oADAMBAAIQAxAAAAHo4uTcqxUf/8QAGhABAAIDAQAAAAAAAAAAAAAAAQACAxESE//aAAgBAQABBQL1Rc1t0yqck5rNE//EABYRAQEBAAAAAAAAAAAAAAAAAAEQMf/aAAgBAwEBPwFRyf/EABcRAQADAAAAAAAAAAAAAAAAAAEAAhH/2gAIAQIBAT8BpVHVmz//xAAYEAADAQEAAAAAAAAAAAAAAAAAATEQQf/aAAgBAQAGPwI5kREQ/8QAGxAAAgIDAQAAAAAAAAAAAAAAAAERMSGRwdH/2gAIAQEAAT8hbRKLF/AfJixy8jFyEqk0f//aAAwDAQACAAMAAAAQuy//xAAWEQEBAQAAAAAAAAAAAAAAAAABERD/2gAIAQMBAT8QOgmf/8QAFxEAAwEAAAAAAAAAAAAAAAAAAAERUf/aAAgBAgEBPxB3qRGH/8QAGxABAAIDAQEAAAAAAAAAAAAAAQARITFBcdH/2gAIAQEAAT8QIQ5btlIKFHX2J5aM0nCLsoqrhBArY6IFQfBP/9k='); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="Transformer"
        title="Transformer"
        src="/static/65ac8ec7c076a56f7ce9113e423d0a74/828fb/image11.jpg"
        srcset="/static/65ac8ec7c076a56f7ce9113e423d0a74/ff44c/image11.jpg 158w,
/static/65ac8ec7c076a56f7ce9113e423d0a74/a6688/image11.jpg 315w,
/static/65ac8ec7c076a56f7ce9113e423d0a74/828fb/image11.jpg 630w,
/static/65ac8ec7c076a56f7ce9113e423d0a74/0ede0/image11.jpg 945w,
/static/65ac8ec7c076a56f7ce9113e423d0a74/6a068/image11.jpg 960w"
        sizes="(max-width: 630px) 100vw, 630px"
        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"
        loading="lazy"
        decoding="async"
      />
  </a>
    </span></p>
<p>RNN+Attention 구조에서 RNN 덕분에 이전 단어의 정보를 참고하지만 순차적으로 진행된다. 그러나 Attention 구조에 의해 결국 문장 전체 특징을 볼수 있게 된다. 이 특성은 그럼 RNN을 거칠 필요가 없지 않을까? 라는 의문에 도달하게 되고, 그리하여 RNN을 제거하고 self-Attention만을 강조한 구조가 등장하는데, 이것이 바로 NLP의 획기적인 성능향상을 불러온 Transformer 이다. 또한 Multi-headed Attention이라는 여러개의 인코더를 쌓아올린 구조가 적용되며 큰 성능향상으로 이어졌다.</p>
<p>RNN을 제거한 후, 각 단어 임베딩과 각 단어의 순서를 알기위해 positional embedding을 추가하여 단어별 순차연산이 아닌 문장을 한번에 연산하게 만들었는데, 이는 대량 병렬연산을 가능하게 해주어 학습속도의 향상을 가져왔다.</p>
<p><img src="/5ad7cfada896a7102e9c33e45d11384f/image12.gif" alt="Transformer 과정"></p>
<h2>4. BERT의 등장</h2>
<p><span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; "
    >
      <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 61.39240506329114%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAECAwX/xAAWAQEBAQAAAAAAAAAAAAAAAAAAAgP/2gAMAwEAAhADEAAAAezOqzWMt//EABoQAQEAAgMAAAAAAAAAAAAAAAEAAhIRITH/2gAIAQEAAQUCyeHZjxxG0I6v/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFREBAQAAAAAAAAAAAAAAAAAAECL/2gAIAQIBAT8Bo//EABkQAAEFAAAAAAAAAAAAAAAAAJEAASAxQf/aAAgBAQAGPwLQrcR//8QAHBAAAwEAAgMAAAAAAAAAAAAAAAERITFBUXGR/9oACAEBAAE/Icy68UadCPHtGFdPb9ESRH//2gAMAwEAAgADAAAAELwv/8QAFhEAAwAAAAAAAAAAAAAAAAAAEBEh/9oACAEDAQE/EIh//8QAGREAAgMBAAAAAAAAAAAAAAAAAREAIWHw/9oACAECAQE/ECXpLsgauf/EABsQAQADAAMBAAAAAAAAAAAAAAEAESExQVGR/9oACAEBAAE/EG8irIQ2qg52iVFWLtKYr0cYxZuvpBIUE//Z'); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="Google AI + NLP BERT"
        title="Google AI + NLP BERT"
        src="/static/e341ff5b1c8fa733fbafc4f2dab6e1af/828fb/image13.jpg"
        srcset="/static/e341ff5b1c8fa733fbafc4f2dab6e1af/ff44c/image13.jpg 158w,
/static/e341ff5b1c8fa733fbafc4f2dab6e1af/a6688/image13.jpg 315w,
/static/e341ff5b1c8fa733fbafc4f2dab6e1af/828fb/image13.jpg 630w,
/static/e341ff5b1c8fa733fbafc4f2dab6e1af/a6e84/image13.jpg 869w"
        sizes="(max-width: 630px) 100vw, 630px"
        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"
        loading="lazy"
        decoding="async"
      />
    </span>
이미지출처 : <a href="http://www.aitimes.kr/news/articleView.html?idxno=15036">http://www.aitimes.kr/news/articleView.html?idxno=15036</a></p>
<p>BERT(Bidirectional Encoder Representations from Transformers). 구글에서 2018년 발표한 BERT는 그 당시 압도적인 퍼포먼스로 단번에 NLP와 인공지능의 대표주자가 되었다.</p>
<p>기존 언어모델(LM)은 앞의 단어들을 참조하여 다음에 나올 단어를 예측하는 방식이었다. BERT는 앞에 나온 단어로 다음에 올 단어를 예측하는 것이 아니라 문장의 중간 단어를 마스킹한 후 전체 문장에서 해당 단어를 예측하는 방식으로 학습된다. 이를 Masked Language Model(MLM)이라고 부른다.</p>
<ul>
<li>기존 언어모델(LM)훈련 - 순차적인 예측이고 이후의 정보는 참고할 수 없다
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">> BERT는 고성능의 획기적인 (예측) => 언어
> BERT는 고성능의 획기적인 언어 (예측) => 모델
> BERT는 고성능의 획기적인 언어 모델 (예측) => 이다.</code></pre></div>
</li>
<li>BERT의 MLM훈련 - 중간 mask된 단어를 문장 전체를 보고 예측한다
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">> BERT는 고성능의 (mask) 언어 모델 이다. => 획기적인
> BERT는 (mask) 획기적인 언어 모델 이다. => 고성능의
> BERT는 고성능의 획기적인 (mask) 모델 이다. => 언어</code></pre></div>
</li>
</ul>
<p>그리고 두 문장이 이어지는 관계인지 아닌지를 학습하는 기능을 추가하였다. Next Sentence Prediction(NSP)라 부른다.</p>
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">> BERT는 고성능의 획기적인 언어 모델 이다. 모든 테스트에서 최상위 성적을 내었다.
NSP 결과 : 1 (이어진 문장)
> BERT는 고성능의 획기적인 언어 모델 이다. 난 아무래도 천재인가봐.
NSP 결과 : 0 (이어지지 않는 문장)</code></pre></div>
<p>BERT는 문장을 생성하지 않고 문장을 분석하고 이해하는데만 집중하는 모델로 Transformer 구조에서 디코더를 생략하고 인코더만 이용했다.</p>
<p><span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; "
    >
      <a
    class="gatsby-resp-image-link"
    href="/static/8cea6e003e3e0bdb43e59476a125e600/6a068/image14.jpg"
    style="display: block"
    target="_blank"
    rel="noopener"
  >
    <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 56.32911392405063%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAGQAAAgMBAAAAAAAAAAAAAAAAAAQBAgMF/8QAFQEBAQAAAAAAAAAAAAAAAAAAAwL/2gAMAwEAAhADEAAAAejR6BTIZGj/xAAaEAACAgMAAAAAAAAAAAAAAAABEgIDABAT/9oACAEBAAEFAjYQepau1wscWOv/xAAZEQABBQAAAAAAAAAAAAAAAAAAAQIRElH/2gAIAQMBAT8Bs3BYP//EABcRAAMBAAAAAAAAAAAAAAAAAAIQElH/2gAIAQIBAT8Bg9X/xAAYEAADAQEAAAAAAAAAAAAAAAAAARAxEf/aAAgBAQAGPwKdMRin/8QAGhABAQEAAwEAAAAAAAAAAAAAAREAMVHB8f/aAAgBAQABPyFYE5wNdzIsBGc5Z8t8rQ63/9oADAMBAAIAAwAAABDUH//EABcRAQEBAQAAAAAAAAAAAAAAAAERACH/2gAIAQMBAT8Q4TEmhN//xAAWEQADAAAAAAAAAAAAAAAAAAAAARH/2gAIAQIBAT8QVKUf/8QAHBABAAMBAAMBAAAAAAAAAAAAAQARITFRcYGx/9oACAEBAAE/EMYIq75mEW0LbX7k0BWj6S4KKqqIACA5wQLgPk//2Q=='); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="BERT"
        title="BERT"
        src="/static/8cea6e003e3e0bdb43e59476a125e600/828fb/image14.jpg"
        srcset="/static/8cea6e003e3e0bdb43e59476a125e600/ff44c/image14.jpg 158w,
/static/8cea6e003e3e0bdb43e59476a125e600/a6688/image14.jpg 315w,
/static/8cea6e003e3e0bdb43e59476a125e600/828fb/image14.jpg 630w,
/static/8cea6e003e3e0bdb43e59476a125e600/0ede0/image14.jpg 945w,
/static/8cea6e003e3e0bdb43e59476a125e600/6a068/image14.jpg 960w"
        sizes="(max-width: 630px) 100vw, 630px"
        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"
        loading="lazy"
        decoding="async"
      />
  </a>
    </span></p>
<p>이러한 방식으로 많은 데이터를 학습하여 하이퍼 파라미터 값을 생성해 놓았더니, 각각의 독립적인 분류, 추론, 문장비교, 질문대답 등의 테스크에서 간단한 레이어를 추가하고, 적은 데이터와 학습시간으로 미세조정(Fine-Tuning)만 거쳐도 기존의 각 테스크별 SOTA(현 최고성적) 모델들을 압도하는 성능을 보여주었다. 이를 전이학습(Transfer Learning)이라 부른다.</p>
<p>BERT는 출시당시는 상당히 거대한 모델이어서 학습하는데 구글의 TPU를 사용해도 수일에서 수주가 걸리는 학습시간이 필요했다. 하지만 미리 학습된 기본모델과 소스코드까지 구글에서 오픈소스로 공개하였기 때문에 자유롭게 가져다 쓸 수 있다.</p>
<h2>5. BERT의 구조</h2>
<div style="overflow-x:auto;">
<table>
  <tbody>
    <tr>
      <th scope="row">입력</th>
      <th scope="col"></th>
      <th scope="col">나</th>
      <th scope="col">는</th>
      <th scope="col">생각</th>
      <th scope="col">한다</th>
      <th scope="col">.</th>
      <th scope="col">고로</th>
      <th scope="col">나</th>
      <th scope="col">는</th>
      <th scope="col">존재</th>
      <th scope="col">한다</th>
      <th scope="col">.</th>
    </tr>
    <tr>
      <th scope="row">토큰</th>
      <td>(CLS)</td>
      <td>103</td>
      <td>95</td>
      <td>227</td>
      <td>16</td>
      <td>(SEP)</td>
      <td>837</td>
      <td>103</td>
      <td>95</td>
      <td>1015</td>
      <td>16</td>
      <td>(SEP)</td>
    </tr>
    <tr>
      <th scope="row">세그먼트</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th scope="row">포지션</th>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>3</td>
      <td>4</td>
      <td>5</td>
      <td>6</td>
      <td>7</td>
      <td>8</td>
      <td>9</td>
      <td>10</td>
      <td>11</td>
    </tr>
    <tr>
      <th scope="row">BERT</th>
      <td colspan="12">
        base : 12 hidden layer, 768 hidden size, 12 multi-head -&gt; 110M(1억1천만) 파라미터<br/>
        large : 24 hidden layer, 1024 hidden size, 16 multi-head -&gt; 340M(3억4천만) 파라미터
      </td>
    </tr>
    <tr>
      <th scope="row">출력</th>
      <td>O₁</td>
      <td>O₂</td>
      <td>O₃</td>
      <td>O₄</td>
      <td>O₅</td>
      <td>O₆</td>
      <td>O₇</td>
      <td>O₈</td>
      <td>O₉</td>
      <td>O₁₀</td>
      <td>O₁₁</td>
      <td>O₁₂</td>
    </tr>
  </tbody>
  <caption>BERT의 문장 입력시 변환되는 3가지 벡터의 예시이다. 실제 데이터와 차이가 있을 수 있다.</caption>
</table>
</div>
<ul>
<li>문장을 토큰화 해서 전체 문장벡터를 만든다. 문장 시작은 (CLS), 문장끝은 (SEP)이라는 특수한 토큰을 표시한다. 이외에도 몇가지 특수토큰이 존재한다.</li>
<li>문장별로 구분하는 Segment Embeddings을 만든다. 첫번째 문장 0, 두번째 문장 1</li>
<li>각 토큰의 위치를 표시하는 Position Embeddings를 만든다. 이는 Transformer의 positional embedding과 다르다.</li>
<li>이 세가지를 합산하여 입력으로 전달된다.</li>
<li>각 토큰에 해당하는 출력벡터가 출력된다.</li>
<li>이후에 출력값에 적당한 레이어를 추가하여 값을 가공하여(fine-tuning) 원하는 태스크에 적용하게 된다.</li>
</ul>
<h2>6. BERT의 학습</h2>
<p><span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; "
    >
      <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 91.13924050632912%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAYAAABb0P4QAAAACXBIWXMAAAsTAAALEwEAmpwYAAAD/klEQVQ4y32U+2/aVhTH+Vsn7af9vmmqJm1ru3Zat67NljRd+sira5ulaZdmCXk6BgzYvGobY2wCJISHwUBCHkAa8plMmmqaph3pXN+r6/s533PuwwfQbrdJ6jp79QYNRUL7/ivU51NoP9+kujRPvXNEvV7HLZfIjdwi/vAe+sQvFMZ/wnVqNFyXs7MzD4Wv2+3iuC6R2WlKu3u0xTXMxXmU1RXMoEDj5SStbh+3dUDbNtl/8ZTg6wUSaytU5yZpFQu47RatVusS2OueEk1q+GefsR6MEZl+Smxugc2/ZCLLAsnRu6zGdAQlhbiyRurBPYSQihgwSI6OsL0hIkR06k6NweACX7/XIxBW2EqFEQwd5dko0fk/CVdBFmMoI9/iT8ishNfZXH5NcuwmYnafiNkneu8HxICfLdmmVq18UNg7RU6YrIX2kKw+iZfLJBZFonugiBn0R7NECyClO0QDFurDKcJ2n1j2BH1iGimyy6aco1jI0+v1LlOWkybBVJ1o/gx13k9icZvgzjmJkMXqN9cZnw8w80ZCCWUxHs8StntE0oekH8+QTNUQlPwQ6O2Hr9c9QU5aCMkOUh7evdomvhgklB8QD+XYun2f3zfyLEk1ZGmX9JM5pB2QMx3ST54jJ1qI8SLVyj4XFxdeyj2iskIiOMM7zU968mvU1/NESpDcEtibuIZmRsikXqFtzWPc/5ykHkQ2u+ijtzFDzwgmdqhWq5fAfv8MMSAgvr1OQBhHe/gZ0ZkJltUu4aU3GD9+wrY4jbB0A+HFXfQ7nyJJz1mRysTvfIny9jtWxCS1K+DJyTGFYom4YpDKVsi8mMPy+8k6bbKRGOb4GGrOIZG0MUIxsr89IGVVsAplrEfjpIJxcrtl3EaDwWCA7/j4mGqphL21gRVXyPx6h/If0xz1etQ2l8ndukZB10ivr2L5l7BvfEEhKtGu7FO4ex39zSsOOx2azSaDwTm+o6Mj6k6deqVCs9nCsUwqU2PUXz7FmRqjqoSpu02c/TIt16W1ssD+4xHKk6NUlxdwarXhLXFd91Jh5wPdi5LL5dAzGTK2jarpmLkc2Z08Bwctjo4PsewcmpklY1nohkE2X8DMWkOYxzg/P/dqeEKj0eDw4IBCoYCuaeRsG8uyMNIGpb0codA+bxdLVCp51JQ6nM9mTTKGMRThwTwfborXeBE8qKfWK4FhGEiSRDweR1XfsbERRBBi6LqKqmooioJt25yennJ4eIjjOMP+8Op5jZe7B/NqceW1Wo1isTgMVq87NJuN4RN2pcZz7z/v6fPOsmcfFf6XeRFFUUSWZQRhm/X1ddbWVtE07ePif9rV2Hc1+LdfmVfo9+/fD7/9fh/vmP3fmr8BhXLyojHD6vEAAAAASUVORK5CYII='); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="BERT의 학습"
        title="BERT의 학습"
        src="/static/9f7b3c092c4d34db48d25981344f52f6/f058b/image15.png"
        srcset="/static/9f7b3c092c4d34db48d25981344f52f6/c26ae/image15.png 158w,
/static/9f7b3c092c4d34db48d25981344f52f6/6bdcf/image15.png 315w,
/static/9f7b3c092c4d34db48d25981344f52f6/f058b/image15.png 630w,
/static/9f7b3c092c4d34db48d25981344f52f6/37523/image15.png 720w"
        sizes="(max-width: 630px) 100vw, 630px"
        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"
        loading="lazy"
        decoding="async"
      />
    </span>
이미지출처 : <a href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a></p>
<ul>
<li>입력에 대하여 15%내의 토큰을 masking하여 모델이 내부 양방향 연산을 통하여 적절한 단어(토큰)을 예측하여 문장을 완성한다. 실제 단어와 비교하여 학습.</li>
<li>또한, 두 문장일 경우 뒤 문장이 앞 문장의 연결문장인지 여부를 판단하여 학습.</li>
<li>영어기준 30,000개의 단어 토큰을 사용하였다.</li>
<li>학습데이터는 총 3.3B(33억) corpus</li>
<li>입력 시퀀스의 최대 토큰은 512개로 제한하였다.</li>
<li>batch size : 256시퀀스 * 512토큰 = 128,000토큰/batch 1M STEP -> 40 epochs</li>
<li>learning rate : 1e-4, 𝛃1=0.9, 𝛃2=0.999</li>
<li>BERT-base : 4개의 TPU를 이용하여 4일간 학습</li>
<li>BERT-large : 16개의 TPU를 이용하여 4일간 학습</li>
</ul>
<h2>7. BERT의 활용</h2>
<p><span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; "
    >
      <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 94.9367088607595%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAIAAAAf7rriAAAACXBIWXMAAA7DAAAOwwHHb6hkAAADiklEQVQ4y12SWY/jVBCF/feRkBBoxAsgIZ4QmulWNxpoppmsju3Y8b7HWZzEHcdb4nhJvN1bKN2ABEf1VlU6pfMVsXPdS3kFQBh1GMFrdRghjDEAIITWi0XXdBgD6gCjFqMGdx3qbl1CGvQ5gePN3Uy3OVXmVJnX16xqGfMlABziI/XHr4LC8qYgGDJveLzhi+acVdgoPhID3iQNd2KeSd3vCWZfsinzONIPtLwAQKut31OcibEYqfOxtpwYBW23tJ18Fu2NdyAmakLaQJmXp2l0N9iTFlBWM5nDzAgBN64XDdSatGBkwPvey+/0jjELyup6WuP5EcEwn+TZPaMfWIHlpo/C7COnqJwsziQRANytN2M+GPIHVXivCA8T/UTaWNJ4aXZ3c5bYnkT+ImpzmScF8l4Y38kiy7NjU1cAIAgOzMd38vM3Su9bZfCDoK14O5amz7NP30fxiSiD5KAvqjxLd/vQXkdzNztEp93+es5vaXcoMFY7wdqry9Bym+oKuE29QzTf3tIGgLZr/TCo6vo2jXF8jIuyhP+qaZsgDg5hUNd1hxGGV1RpmsqyrKqqruuqqsqvchznbadtW8MwVEVRFcVdr5fLheu6+/0+SZLbMkLIcRxJltbrdZ5lpzSN0zR/dX77k7Is0yI/5zmG/4uoq7q61tWr6qa9luXBNj1dy46n27VNc63qMgiP3ou/dsskLpM4C4M8DDEAIZrmUJxR2o7S1mNZGHEU12eYP2maEwBgvduPZZqZ9miuz8mzqeizYjCV5DHdz8qKmJn7oZ5QVj7WTk/T1VAJ6cV14lQjcQOoXe6SgRLQzpm0ks+SP9ILxmlp59qX/Cy/EKIdTfSSsco+Hz705pReMGbB2BUl7wE1K+80VHLarMZK9jBYfuYDxi4ZqxqKUVZcCFl44sdfTwRtKjAS86NAfcfOegw3ntK/IYDlaqGQXyz4rxzuS0f+aaQVQ63S+J8F8t0xawlvudCpsbvaLu2FMaUMll6Y1lw33fkcAJIwlJ7v9f6j3n80Js/b/dELznNuqk+GTYeJtihLP7icz5fT6RJGdZTkUZTHUVu80kKoCpPcO6Tbfer5gLsbvCi+xjcWBAZAgIMoPJ3PHcYtQkEcnbIz/AO6xRgBpEXux2EQBuXl0rTd35yLopAkSdO0zWbz8vKy3WyKIv/3QxBClmUpsizwvCiKtm17nhfH8dvAX9ke7j6GyAjMAAAAAElFTkSuQmCC'); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="BERT의 활용"
        title="BERT의 활용"
        src="/static/4542f3429e906153a581351afc65e26c/f058b/image16.png"
        srcset="/static/4542f3429e906153a581351afc65e26c/c26ae/image16.png 158w,
/static/4542f3429e906153a581351afc65e26c/6bdcf/image16.png 315w,
/static/4542f3429e906153a581351afc65e26c/f058b/image16.png 630w,
/static/4542f3429e906153a581351afc65e26c/35751/image16.png 873w"
        sizes="(max-width: 630px) 100vw, 630px"
        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"
        loading="lazy"
        decoding="async"
      />
    </span>
이미지출처 : <a href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a></p>
<ul>
<li>기본적으로 두 문장의 연관성파악이나 단일문장의 분류문제, 태깅문제, 질문과 답 문제 등에 적용가능</li>
<li>BERT 모델(레이어) 위에 추가적인 레이어를 올려서 다른 새로운 태스크 에도 적용해 볼 수 있다</li>
</ul>
<h2>8. BERT의 성능을 높인 기술들 : RoBERTa, ALBERT, DistilBERT, ELECTRA</h2>
<h3>8.1. RoBERTa (A Robustly Optimized BERT Pretraining Approach)</h3>
<p>BERT의 파라미터 및 트레이닝 방법의 변화를 통해 성능을 향상시킨 버전이다.</p>
<ol>
<li>더 많은 데이터를 사용하여 더 오래, 더 큰 batch로 학습</li>
<li>NSP(next sentence prediction) 제거</li>
<li>짧은 sequence는 배제하고 더 긴 sequence로 학습하기</li>
<li>static masking을 dynamic masking으로 바꾸기</li>
</ol>
<h3>8.2. ALBERT (A Lite BERT)</h3>
<p>A Lite BERT For Self-Supervised Learning of Language Representations
BERT에서 사이즈가 크다고 무조건 성능이 향상되지 않는다는 것을 증명하며 등장한 버전이다.</p>
<ol>
<li>큰 파라미터는 OOM(out-of-memory)문제, 학습시간 증가를 야기 => input token embedding 사이즈를 줄여서 전체 파라미터를 줄임(Factorized Embedding)</li>
<li>Transformer layer 간 같은 Parameter를 공유하며 사용</li>
<li>NSP(next sentence prediction) 대신 SOP(Sentence order prediction) 사용</li>
</ol>
<h3>8.3. DistilBERT (a distilled version of BERT)</h3>
<ul>
<li>KD(Knowledge Distillation)라는 압축기술을 이용하였다.</li>
<li>KD는 큰 모델을 선생(Teacher)으로 작은 모델인 학생(Student)을 학습시킨다.</li>
<li>BERT의 경우 크기는 40%감소, 속도는 60%증가, 결과치는 97%유지되었다.</li>
<li>학생은 선생의 출력결과의 확률 분포를 배움으로써, 특히 0에 가까운 출력값의 복잡한 특징 신호를 단순한 구조로도 배울 수 있게 되어 성능하락이 거의 없이 빠르다.</li>
<li>ALBERT도 일종의 압축모델이지만, 애초에 구조를 줄여서 트레이닝 시키는 형태이고, DistilBERT는 이미 사전학습된 BERT를 압축하는 구조이다.</li>
</ul>
<h3>8.4. BART (Bidirectional Auto-Regressive Transformer)</h3>
<ul>
<li>BERT가 분류등의 일반 테스크 성능은 뛰어나지만, 문장생성 등의 테스크에서는 매우 취약함을 보이는데, 이는 디코더를 사용하지 않기 때문이다. 이를 보완하기 위해서 BART는 BERT방식에, 뒤에 소개될 GPT의 디코더 구조를 사용하여 나머지 성능은 유지하고, 문장생성, 지문해석 등의 태스크에서 큰 성능향상을 보여준다.</li>
<li>기본원리는 denoising autoencoder 방식으로 사전학습이 되는데, BERT의 MLM방식처럼 입력 텍스트에 일정부분 변형(noise)을 가하고, 이것을 원래대로 복구하는 방식으로 학습이 진행된다. BART의 특징은 BERT는 단어 하나를 masking 하였는데, BART에서는 어떤형태의 변형(noise)방법이든 적용 가능 하다는 것이다.</li>
<li>base 모델은 6개의 인코더와 6개의 디코더 레이어를 가지며 인코더에서 디코더로 넘어갈 때 cross-attention를 수행한다. BERT보다 대략 10%정도 많은 파라미터 수를 갖는다.</li>
</ul>
<p><span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; "
    >
      <a
    class="gatsby-resp-image-link"
    href="/static/e017c63d3fd4d1cfc636550379ee63d3/38cea/image17.png"
    style="display: block"
    target="_blank"
    rel="noopener"
  >
    <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 29.11392405063291%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAIAAABM9SnKAAAACXBIWXMAAAsTAAALEwEAmpwYAAABRElEQVQY0yXQyW6CQAAAUP7/B0yjjSG10RZN46FJrz1oouKCVUZ2RnBAFOvOrEzT9vqOT6Gn0329xgiRDSqSpORcSknO5yuE9zi+QPiLQvzjLYpuUVRs4nMUC86VoN+f1Gr26wtoPIF2h91umDGn3xtVq1/Pjf5DBWjtwzblUvqD4bxeH9frc1X9rFS+s0xJtunKcfwwWJgmsB2MMULxygQIhpPhYjwwosBbLs1dlvpB4Pp+AKEXhivH9sNQkRiz/Z7mOT8cxOlICMMFx2W5JUVviXQ3zQWjZVkUnDNO8pzudmS/J1kmikJJ9RFQVavZXKpq+vEOvXQ2QeZh9zidvcFA89yW663Pl+k4QWHqddpJt+trmt9qHS1LiePImM1syxrrIwAAY0xKKYSgjBFKCaWUsbIsxd+ZY1uGMdeHw5GuX6/XH5aAOmoltuAKAAAAAElFTkSuQmCC'); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="BART"
        title="BART"
        src="/static/e017c63d3fd4d1cfc636550379ee63d3/f058b/image17.png"
        srcset="/static/e017c63d3fd4d1cfc636550379ee63d3/c26ae/image17.png 158w,
/static/e017c63d3fd4d1cfc636550379ee63d3/6bdcf/image17.png 315w,
/static/e017c63d3fd4d1cfc636550379ee63d3/f058b/image17.png 630w,
/static/e017c63d3fd4d1cfc636550379ee63d3/38cea/image17.png 678w"
        sizes="(max-width: 630px) 100vw, 630px"
        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"
        loading="lazy"
        decoding="async"
      />
  </a>
    </span></p>
<h3>8.5. ELECTRA</h3>
<p>Efficiently Learning an Encoder that Classifies Token Replacements Accurately</p>
<p>BERT에서 학습의 효율성에 기반하여 새로운 pre-training 방식을 제시한 버전이다.</p>
<ol>
<li>Masked Language Modeling(MLM) 대신 Replaced Token Detection(RTD) 사용</li>
<li>masking 대신 작은MLM이 대체단어를 생성하면 모델은 이 가짜를 구별해 내는 방식</li>
</ol>
<p>오리지날 BERT가 샘플 한 문장당 최대 15%의 단어만 masking하여 학습하므로 데이터셋에 대한 학습률이 15%였던 MLM방식과 다르게, RTD를 이용함으로써 샘플 한 문장을 부분적으로 단어가 바뀐 여러문장으로 생성이 가능하므로, 원본 문장의 전체 단어 모두를 각각 바꿀 수 있기 때문에 한문장의 모든 단어를 100% 사용 가능하게 된다. 이는 준비된 데이터셋의 크기대비 훈련효율을 향상시킨다.</p>
<p><span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; "
    >
      <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 65.82278481012659%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsTAAALEwEAmpwYAAACuUlEQVQ4y12TTW8TVxSG/Tf6JypWIHVRKgFbFhX7skKVuuqiICGgaqWWEmhUpWqFCERQQAEVXBoIJLDIR4NQElyTYuXD2I6nib/tsT3fM3fuPNXccVyFIx3dK90zz33fM/ekambEVlui9STlrqSkS1p2RN2MkBEqoijat1b6oPWgaiRr3QStC6GElIwihEwyjrYtmd70+XHJ5WHOjzHEnD1YIF3q1jsadomaVaJqFqhZRSpmHivQSQ00kNRHfPHY4cNfHQ5ec/lgxCJXF6pCSKlWO9B5Wb1LpvEnrxtpXtUmyTanWKzcpGZtkopv3rs9lCEnJm0+uuHx8YTL4esmNSNUZ3JQ44cW2eYzss0nZBpTCrzWmma5nqblbCcKB7Uq7mQE38+7XFzwhrAElChsOxqXM0e5nDnGeO4zrudO8vObT7nw6gDLtXtxDyUyCgkjF61XoNgpxQZjvaoFcaTXQ05NeWpvBR2ela/wuPQdczvjLOxOMFse5VHxa0q9lcRyHF5osVS9xd/N9FBxECabq6uCY795Q9umEPgS8t2XyroV9Ol4gfouZQddds0cVWsDzVinam1RsdZV+qEzsBvxb08OFdsiVMBib4W37efYwkD3RQJcaz3l2+VDjLw+wi9rJ/gpe5yLq4f5YfUTBU3if1jfb/B7/gz3818xvX1J2U8XznN743M29DlS5X6G+/nTPCp+w6w2xkx5lHThHH8UzqO7O4M/LFXuAR+8O8vT7REWdyeY3xnnhTbG5NaXbOrzSQ+NIMALXTb0BTQjgxMKur47nI73J8VSPZRoRpZNfRFbmOi+n1iWkaDlGvT9Dn9V7vCm+YSeb1K1O8rq3pQkmTz+jtfHDBz+ac2wVLlL16uza+sIGcTAkIbTQ0hB2/NxwwgzsKna+r6x2w80sIWjlHZ9oZ6dZrYU4z/6JbY3/vtthgAAAABJRU5ErkJggg=='); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="ELECTRA"
        title="ELECTRA"
        src="/static/7687f6a2cb83d82ae247184bc03ae7c1/f058b/image18.png"
        srcset="/static/7687f6a2cb83d82ae247184bc03ae7c1/c26ae/image18.png 158w,
/static/7687f6a2cb83d82ae247184bc03ae7c1/6bdcf/image18.png 315w,
/static/7687f6a2cb83d82ae247184bc03ae7c1/f058b/image18.png 630w,
/static/7687f6a2cb83d82ae247184bc03ae7c1/681f1/image18.png 899w"
        sizes="(max-width: 630px) 100vw, 630px"
        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"
        loading="lazy"
        decoding="async"
      />
    </span>
이미지출처 : <a href="https://littlefoxdiary.tistory.com/41">https://littlefoxdiary.tistory.com/41</a></p>
<h2>9. BERT를 이용한 서비스</h2>
<p>BERT는 뛰어난 성능에도 불구하고 실제 서비스에 적용하기에 몇가지 문제가 있다.
기본적으로 너무크고 연산이 많아서 latency가 늦어져 실시간 서비스에 적용하기에 제한적이다.</p>
<p>이러한 문제점을 극복하고 실 서비스에 적용한 로블록스 사례를 보자.</p>
<h3>9.1. ROBLOX</h3>
<p>메타버스 게임회사의 대표 로블록스에서 하루 10억건 이상의 게임유저의 텍스트를 분류 처리하는 시스템에 BERT를 적용하였다.</p>
<p>기존 시스템을 BERT로 변경하며 텍스트분류에서 10%이상의 성능향상을 달성
하지만 하루 10억건이 넘는 데이터를 latency 20ms 이하로 처리해야 하는데 BERT는 너무 느렸다. BERT 기본모델의 latency 측정치 330ms. 이에 3~4가지의 튜닝을 거쳐서 GPU없이 latency 10ms를 달성한 방법을 공개하였다.</p>
<p><span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; "
    >
      <a
    class="gatsby-resp-image-link"
    href="/static/16b46889a0d277ec8d188e3ceeb219e6/76823/image19.png"
    style="display: block"
    target="_blank"
    rel="noopener"
  >
    <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 53.79746835443038%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAABqElEQVQoz22Sv47UMBDG8xC0SHS8BA2i4wXoTkI0vMEh0CEhajpaoEFIgKh4h7sDsVdwSBQr0LIXEjvZjbNJnNjzz8jJ5grg0xRja2b8+zROwj9q29YYs5tl6tr2ffifEmaessZS13XWdlrrNE210lmW5XlWpmlRFP0wtG3rnJ+KmVlEkmEYpoMD9kjeeyJiZhoVizDKAxBBEApBQpCpLPE+DqNhAJXzMOKJxJh1mbYu/CpFGYkDAYg5QcR9ESJsSthuhOivETQ6e/q2v37f3H7SFLWEEKn22DIpBOqtVznWJgKPiMyMGLuPXturd83NhztlWBgQR2yRSDJGbEDnndau0GAqnkyNLz9+01+7V916FJsjJ3HinJuoZ4eyz4Wpt04pc6HstgnBPXjZXDlobxw2ykhsRoqeAeBn7s+W9vcGEGGxdB9PmsXSEfOH4/7oVfn8veZKvXi3vnP4/eCZUZUXBpF5z2mmv5ydm10XQlhd6OPPX9dpMd6Xp4tvP9Z5CMFU2/PFp9VqNa02fpJ56QjgmHDkRcJ9zgRMnjA6B+9NXfe2u/xhfwAJbWufE08zugAAAABJRU5ErkJggg=='); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="How We Scaled Bert To Serve 1+ Billion Daily Requests on CPUs"
        title="How We Scaled Bert To Serve 1+ Billion Daily Requests on CPUs"
        src="/static/16b46889a0d277ec8d188e3ceeb219e6/f058b/image19.png"
        srcset="/static/16b46889a0d277ec8d188e3ceeb219e6/c26ae/image19.png 158w,
/static/16b46889a0d277ec8d188e3ceeb219e6/6bdcf/image19.png 315w,
/static/16b46889a0d277ec8d188e3ceeb219e6/f058b/image19.png 630w,
/static/16b46889a0d277ec8d188e3ceeb219e6/40601/image19.png 945w,
/static/16b46889a0d277ec8d188e3ceeb219e6/76823/image19.png 1038w"
        sizes="(max-width: 630px) 100vw, 630px"
        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"
        loading="lazy"
        decoding="async"
      />
  </a>
    </span>
(그래프설명: 각각 BERT-base, DistilBERT, +가변형 입력, +가중치 양자화)</p>
<p>이미지출처 : <a href="https://medium.com/@quocnle/how-we-scaled-bert-to-serve-1-billion-daily-requests-on-cpus-d99be090db26">https://medium.com/@quocnle/how-we-scaled-bert-to-serve-1-billion-daily-requests-on-cpus-d99be090db26</a></p>
<ol>
<li>CPU / GPU 학습은 병렬연산이 필수적이지만, 실 서비스에 적용하는 것은 각각의 단일 문장의 요청에 대한 처리 이므로 CPU모델을 성능기준으로 잡았다. 여기서는 Tesla V100 GPU와 동급 가격인 36코어 Xeon CPU를 사용하였다.</li>
<li>DistilBERT 기술을 이용하여 모델을 작게 만들어 속도를 2배 향상시켰다.</li>
<li>단일문장의 입력이므로, 0을 padding하여 동일한 입력크기로 맞추지 않고 각 문장까지만 그대로 입력하는 Dynamic Shapes를 사용하여 2배 향상을 이뤘다.</li>
<li>32비트 부동소수를 8비트 정수로 변환하는 가중치 양자화 기술을 사용하여 연산의 효율성을 획기적으로 높였는데, 그중 Dynamic Quantization 이라는 훈련후 가중치를 양자화 하는 기법을 사용하여 8배 정도의 속도향상을 달성하였다.</li>
<li>그외 입력 데이터의 caching 기법 등으로 추가적인 성능 향상이 있지만, 위 1~4의 과정 만으로 약 30배의 성능 향상을 이루어, 수평적인 CPU 확장만으로 하루 10억개의 테스크 처리가 가능하였다.</li>
</ol>
<h2>10. BERT의 사촌들 : GPT, XLNet, T5</h2>
<p><span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; "
    >
      <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 43.67088607594937%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsTAAALEwEAmpwYAAABO0lEQVQoz42SW4vVMBhF+/9/niIiCIOj0suZk3zN/TZdkqYeR5DBh0V2197JUyfvPe4/CCGQY+a9fe8miYa9WMwb9ovf2RbPlu58NF/Rue/dY/MWSaY/2D8i5qJnWyOh/XG2JFS2fI8zvgVCS4+u73wd93RyTBIKpjDIB64crOHgi+4Z9nxgy8FLhA8vnP6z7g4kwyd18GTGVsfKZF3Ch0oIBR8rMTXEFX7q8nAhNYwvPKvMLFcXKz4UfujCzRRifsX4zFSshRAgJdhuHOs6crzcunHMy8gpXD6MO+e9CN7Bt2eaCFNxjmYtzXnq/U7dblTnqNby6j3tLzd8P5sbnM5Y2ryQtTAty4LWGq0U+76jRUYWYZlnVO9EEBGUUufZ97dtY1u3M5+d1hhjmKy15//zL97rnHMnD3flXxOOtmzazVMYAAAAAElFTkSuQmCC'); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="After BERT &amp; ELECTRA 언어모델 비교 및 선정"
        title="After BERT &amp; ELECTRA 언어모델 비교 및 선정"
        src="/static/8ed92ff74b782beac927b30fe77345a1/f058b/image20.png"
        srcset="/static/8ed92ff74b782beac927b30fe77345a1/c26ae/image20.png 158w,
/static/8ed92ff74b782beac927b30fe77345a1/6bdcf/image20.png 315w,
/static/8ed92ff74b782beac927b30fe77345a1/f058b/image20.png 630w,
/static/8ed92ff74b782beac927b30fe77345a1/40601/image20.png 945w,
/static/8ed92ff74b782beac927b30fe77345a1/78612/image20.png 1260w,
/static/8ed92ff74b782beac927b30fe77345a1/21b4d/image20.png 1280w"
        sizes="(max-width: 630px) 100vw, 630px"
        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"
        loading="lazy"
        decoding="async"
      />
    </span>
이미지출처: <a href="https://mysterico.tistory.com/8">https://mysterico.tistory.com/8</a></p>
<h3>10.1. GPT (Generative Pre-trained Transformer)</h3>
<p>OpenAI라는 단체에서 Transformer 구조에서 인코더는 무시하고 디코더 부분만 집중하여 문장생성모델을 만들었는데, 이것이 GPT이다. 대량의 문서를 학습하여 어떤 단어가 주어졌을 때 다음에 올 확률이 가장 높은 단어를 제시하여 순차적으로 문장을 만들어 나간다.</p>
<p><span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; "
    >
      <a
    class="gatsby-resp-image-link"
    href="/static/749d3c42df16619296070409640e8f83/6a068/image21.jpg"
    style="display: block"
    target="_blank"
    rel="noopener"
  >
    <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 56.32911392405063%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAIEBf/EABUBAQEAAAAAAAAAAAAAAAAAAAEA/9oADAMBAAIQAxAAAAG9NFVlLSP/xAAaEAACAwEBAAAAAAAAAAAAAAAAAQIDERIT/9oACAEBAAEFAnez3mV2PHFM4iYj/8QAFxEAAwEAAAAAAAAAAAAAAAAAARARQf/aAAgBAwEBPwEzF//EABYRAAMAAAAAAAAAAAAAAAAAAAEQMf/aAAgBAgEBPwERf//EABkQAQEBAAMAAAAAAAAAAAAAAAEAMTJBcf/aAAgBAQAGPwJAxup9suJZf//EABsQAQACAgMAAAAAAAAAAAAAAAEAMSGBEBHh/9oACAEBAAE/ISoVGc8BfYEWQdQ8SBUDU//aAAwDAQACAAMAAAAQZC//xAAWEQEBAQAAAAAAAAAAAAAAAAABERD/2gAIAQMBAT8QaaIZ/8QAGBEBAAMBAAAAAAAAAAAAAAAAAQARIVH/2gAIAQIBAT8QYUdlnJ//xAAdEAEAAgICAwAAAAAAAAAAAAABABExQSFhcYHh/9oACAEBAAE/EBMVErlnEsyDwfY7WLZV0dwCt9yGesmsHoboT//Z'); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="GPT"
        title="GPT"
        src="/static/749d3c42df16619296070409640e8f83/828fb/image21.jpg"
        srcset="/static/749d3c42df16619296070409640e8f83/ff44c/image21.jpg 158w,
/static/749d3c42df16619296070409640e8f83/a6688/image21.jpg 315w,
/static/749d3c42df16619296070409640e8f83/828fb/image21.jpg 630w,
/static/749d3c42df16619296070409640e8f83/0ede0/image21.jpg 945w,
/static/749d3c42df16619296070409640e8f83/6a068/image21.jpg 960w"
        sizes="(max-width: 630px) 100vw, 630px"
        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"
        loading="lazy"
        decoding="async"
      />
  </a>
    </span></p>
<p>GPT는 학습데이터가 많을수록 상당한 성능 향상을 발견하여 현재 버전3 에서는 인간이 작문하였다고 봐도 크게 이상하지 않을 정도의 문장생성 능력을 보여준다. 다만, 사용된 파라미터만 175B(1,750억)개로 BERT-base 모델 110M(1.1억)개의 1,600배에 달한다.</p>
<p>참고로 OPEN-AI에서 GPT-1이 먼저 나오고 Google에서 더 성능좋은 BERT를 내 놓았고, OPEN-AI에서 GPT-2, 3로 업그레이드 하면서 서로 성능경쟁으로 발전하는 관계이다.</p>
<p><img src="/dae43ccfeec65ce3f9cbd289e529f1af/image22.gif" alt="How GPT3 Works - Visualizations and Animations">
이미지출처: <a href="http://jalammar.github.io/how-gpt3-works-visualizations-animations/">http://jalammar.github.io/how-gpt3-works-visualizations-animations/</a></p>
<h3>10.2. XLNet</h3>
<p>BERT가 각 masking단어별로 독립적이라고 가정하여 생기는 단어간의 위치에 대한 연관성이 무시되는 문제와 fine-tuning시의 masking기법이 쓰이지 않음으로 부조화에 의한 성능저하를 극복하고자 나온 방식이다.</p>
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">Permutation Language Model
[ ‘New’, ‘York’, ‘is’, ‘a’, ‘city’ ]
[ ‘is’, ‘a’, ‘city’ | ‘New’, ‘York’ ]
[ ‘is’, ‘a’, ‘city’  ‘New’, ‘York’ ]
   ┖─────┚              ┃
         ┖──────────┚ </code></pre></div>
<p>pre-training에서 masking을 쓰지않고 단어를 뒤섞어서 순차적으로 예측하는 방식을 여러번 반복하여 학습하는 Permutation Language Model이다. 이는 이후 fine-tuning시에도 기법의 부조화가 없어서 성능향상이 된다. 다만, 실제 작동은 단어를 뒤섞지 않고 Transformer의 self-attention을 활용하여 attention-mask로 구현된다.</p>
<h3>10.3. T5 (Text-to-Text Transfer Transformer)</h3>
<p><span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; "
    >
      <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 36.708860759493675%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAA7EAAAOxAGVKw4bAAABq0lEQVQoz3WR22oaARiE92m8FvRexFfw2bzwriBIE5sLg61RE41N1O12PezBdd11Pet6iLWtiWiiX+kWCr3owDDMzzAw/MJkMUbURERF5IsqIukSsiFT79QR1RqG02a1WnE4HPgvTmfOpzOcQRjbNlIhT71UpFEq/tH7Il/vbr27raq4iwWbzYZCoUAikSCZTDKbzdhut4ymEyRNotqqYA27CN+f1kztLjPHZjFwPJ33bSZ2l4ltMh8NGA4HaJpGJBIhHA7j8/mIxWLsdjuGuo5ee6T5UKZvGAjzsU67kUZvZOgonzyq8jVG8yO9dp6ekWMy7lEufyYYDBIIBLzSeDzOcrnAbDYxZBn5Po+jNxFcd0rbaNExVY9GR/nrTVPhx3bK8/NPXNclGo3i9/sJhUKkUinWmy3ueoP79I2upeA4GoJmjbjIVrnK1/iQr5G6qXCZrXg+dVNlOF3izufevN/Y7/e8vh5ZrdZclyQvn76TSOVEaoqFoJp93l3luMiUeJ8pkkzfkn1UyNU0SnWLyXzJ/uWFt7fTP489HI+ILZMHuY3muDStKfbI5Rd8S9zPWkTUnQAAAABJRU5ErkJggg=='); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="구글 새로운 자연어처리 AI ‘T5’ 성능은?"
        title="구글 새로운 자연어처리 AI ‘T5’ 성능은?"
        src="/static/e48b779581028fec208e6ba3bb26ce62/f058b/image23.png"
        srcset="/static/e48b779581028fec208e6ba3bb26ce62/c26ae/image23.png 158w,
/static/e48b779581028fec208e6ba3bb26ce62/6bdcf/image23.png 315w,
/static/e48b779581028fec208e6ba3bb26ce62/f058b/image23.png 630w,
/static/e48b779581028fec208e6ba3bb26ce62/8c557/image23.png 700w"
        sizes="(max-width: 630px) 100vw, 630px"
        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"
        loading="lazy"
        decoding="async"
      />
    </span>
이미지출처: <a href="https://m.news.zum.com/articles/58361658">https://m.news.zum.com/articles/58361658</a></p>
<p>BERT가 분류나 입력 범위만 출력할 수 있는것에 반해 모든 입력과 출력을 문자열로 하고, 각각의 태스크 역시 입력 문자열에 포함하여 재구성한 모델이다.</p>
<p>예로 “translate English to German: That is good.”를 입력으로 넣으면 “Das ist gut.”를 출력으로 내 놓는다.</p>
<p>이렇게 훈련하기 위해 Wikipedia보다 2배 큰 C4(Colossal Clean Crawled Corpus)라는 잘 정제되고 다양한 새로운 훈련 데이터셋을 만들어 훈련하고 공개하였다.</p>
<p>알려진 NLP의 다양한 테스크 뿐만 아니라 새로운 유형의 테스크에도 쉽게 적응하며 뛰어난 성능을 보여준다.</p>
<h2>11. NLP 딥러닝의 최신 동향 : GPT3 등 초거대인공지능</h2>
<p><span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; "
    >
      <a
    class="gatsby-resp-image-link"
    href="/static/9f344a4b3cf8802a0b0f8da6374977fe/6d2da/image24.png"
    style="display: block"
    target="_blank"
    rel="noopener"
  >
    <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 77.21518987341771%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAIAAABr+ngCAAAACXBIWXMAAAsTAAALEwEAmpwYAAABSUlEQVQoz41S2W6DMBD0/39i1VZVGgIOBuziA+9ZAVWvpBXzZq/HM7uzRlWZWY8BEXIuQNIOAUmMiCCiHICqZsCLT6oaF2TZyMeVU1pElJiZVoo5bltEhgjEAkj7jWn6KaW81w44lwrIzPvRPJ77Cvi/pKrW8gY1ESvSl03z0o7wN7lAmmKvqj4lHxOzfK+al3YoS71rW1VtyCcXVJVYF2D5+cqcuvGWvHbFGzaTIgJwJ07z1Lhb20tFP5dc0fosqvuMbpsyY0jff90dvLpUKvS+zAuWAluWd4a/LgkRfTYZUummuQnQ2Oa5CzHTwyX9laNZtbYlWb8gSpV9RiK5vtUps4/Rz8NHXLe2VfUyZhYpQD5+TW7NtmI/uDBfP9P+rczMj6+2df5sx1M3tv107obO+db5q5sudjzZYN1kh9D2U4iJCHMpdcM7LOByRXqmWtYAAAAASUVORK5CYII='); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="Exponential growth of number of parameters in DL models"
        title="Exponential growth of number of parameters in DL models"
        src="/static/9f344a4b3cf8802a0b0f8da6374977fe/f058b/image24.png"
        srcset="/static/9f344a4b3cf8802a0b0f8da6374977fe/c26ae/image24.png 158w,
/static/9f344a4b3cf8802a0b0f8da6374977fe/6bdcf/image24.png 315w,
/static/9f344a4b3cf8802a0b0f8da6374977fe/f058b/image24.png 630w,
/static/9f344a4b3cf8802a0b0f8da6374977fe/6d2da/image24.png 936w"
        sizes="(max-width: 630px) 100vw, 630px"
        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"
        loading="lazy"
        decoding="async"
      />
  </a>
    </span></p>
<p>NLP의 요즘 트랜드는 대형화이다. 초 대형 모델에 엄청난 데이터의 학습. 데이터가 많으면 많을수록 다방면에서 뛰어난 성능을 보여주는 것을 GPT모델이 버전3까지 발전하며 증명하였다.</p>
<p>GPT3는 약 300B(3,000억)개의 데이터셋으로 사전 학습을 실행하였는데, 96개의 레이어로 하이퍼 파라미터를 175B(1,750억)개 사용하였다. 이는 fine-tuning 없이도 각종 자연어 테스크 벤치마크에서 최고점을 받았다.</p>
<p>하지만, GPT3는 93%가 영어인 거의 영어 전용 모델이라는 문제가 있다.</p>
<p>한국에서는 NAVER에서 하이퍼클로바라는 한글GPT3모델을 만들어 서비스될 예정이다.
하이퍼클로바 또한 300B(3,000억)개의 한국어 데이터셋과 204B(2,040억)개의 매개변수로 설정되었다고 한다.</p>
<p>이정도 규모의 모델은 구축비용이 얼마나 들까? 해외의 분석업체의 발표에 의하면 OpenAI의 구축비용은 대략 150억원 정도로 추산한다.</p>
<p>초거대 인공지능은 성능을 진일보 시켰지만, 대신 초대형 자본을 가진 기업들만 도전할 수 있는 분야가 되어버렸다.</p>
<p>딥마인드(DeepMind)의 고퍼(Gopher)에서 쓰인  파라미터 2800억 개를 시작으로, 마이크로소프트+엔비디아의 Megatron-Turing Natural Language Generation model (MT-NLG)는 파라미터가 5300억 개, 구글의 스위치-트랜스포머(Switch-Transformer)의 파라미터는 1조 6000억 개, 구글 GLaM(Generalist Language Model)의 파라미터는 1조 2000억 개이다.</p>
<h2>12. Multi-Modal로 확장하는 NLP</h2>
<p>텍스트에 국한되어있던 NLP가 다른 채널, 영역들과 합쳐지며 확장하고 있다. 학습 데이터로든 출력 결과물이든 텍스트만으로는 완전히 이해할 수 없는 언어라는 분야의 추가 정보를 위해 이미지, 사운드, 영상등의 여러 정보의 통합은 필연적일 것이다.</p>
<p><span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; "
    >
      <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 43.67088607594937%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAIAAAC9o5sfAAAACXBIWXMAAAsTAAALEwEAmpwYAAACMElEQVQozwElAtr9AJiObLyTd7WMbZ+NaWeEUXGIW4WYdYGVcVlzRMDKvenw/9jj8Nfi7srZ7MDS57vQ66/D3IqHf2JfWbe3tQBLYSWjf1ugbEOeakJpYy01WhpFZCteeEtIYy+7xrP////r9/zh7PjR4e/P3+ususeKmalMST87NyygoJ4ASmMog3xOsoZjmGxImHJTd3A8TmQpXW85XnNArbanfYGGcXd4dHZzhoZ7bm5kMCklGxYPERAMHBkQmpqWAJBmOppuRqh7VquVgqGDaqRvRIVhPlVKNoVlOq2klhgaHwUKDhgaFzIwJkE8LjgxJickHDYyJyIeFJuYlACcZkCmd1GgeFegkYW0nIfPpHuDZ1IaHiWAWD3Cr6BISUY3NjA0My8oJBo0LSE5MyUhHBMvKh4MCQOTkY4AoXZVvJVyr4tqrYtrnYFot5Brj3dbST81h2ZLu62fPD8/NjInODEiODEhMSobKyUXNC4fPjgqRDwsr6ulAItYM5VnQ5psR5ttSJFjPpJjPoteOYhdOI1eNqydkQ8TFgEBARERDyIgGj86MVpURm5mVH50YXRpVMC7sgDAqpnIsqHEr57EsJ/HsaDJtKPFr53Aq5nFsJ7Wz8mGiIiIiYqQkpWUlpehoKCpp6Oqp6Kzr6m/u7Ti4d0A5unr4+bo5Ofq5ejq4uXn4+fp5ejr+fz+/v///f396enq7+7u/////v7+/////////v//////////////69ARuOZ64E4AAAAASUVORK5CYII='); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="COCO 2015 Image Captioning Task"
        title="COCO 2015 Image Captioning Task"
        src="/static/af5e32b7fbecb8ce3276e676d99e41d2/f058b/image25.png"
        srcset="/static/af5e32b7fbecb8ce3276e676d99e41d2/c26ae/image25.png 158w,
/static/af5e32b7fbecb8ce3276e676d99e41d2/6bdcf/image25.png 315w,
/static/af5e32b7fbecb8ce3276e676d99e41d2/f058b/image25.png 630w,
/static/af5e32b7fbecb8ce3276e676d99e41d2/aec65/image25.png 687w"
        sizes="(max-width: 630px) 100vw, 630px"
        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"
        loading="lazy"
        decoding="async"
      />
    </span>
이미지출처: <a href="https://cocodataset.org/#captions-2015">https://cocodataset.org/#captions-2015</a></p>
<h3>12.1. Image to Text</h3>
<p>seq2seq의 기계번역 모델에서 번역하고자 하는 문장을 인코더에 번역된 문장이 디코더에 나오게 된다. 여기서 만약 입력되는 인코더에 문장이 아니라 이미지가 들어간다면? 이런 아이디어로 연결한 것이 이미지를 보고 설명문장을 생성하는 모델이다.</p>
<p>인코더의 RNN 대신에 이미지를 파악하는 CNN모델을 연결하고, 이미지와 그 설명 문장을 쌍으로 학습시키면 이미지 캡셔닝 모델이 완성된다.</p>
<p><span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; "
    >
      <a
    class="gatsby-resp-image-link"
    href="/static/648a51197ffa44148d305aef4e2f4e9e/6a068/image26.jpg"
    style="display: block"
    target="_blank"
    rel="noopener"
  >
    <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 56.32911392405063%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAQBAwX/xAAVAQEBAAAAAAAAAAAAAAAAAAACA//aAAwDAQACEAMQAAAB0a35mlBwqf/EABkQAAIDAQAAAAAAAAAAAAAAAAABAgMREv/aAAgBAQABBQJ2tDukQtbXK3mJiP/EABYRAQEBAAAAAAAAAAAAAAAAAAEQQf/aAAgBAwEBPwFTJ//EABcRAQADAAAAAAAAAAAAAAAAAAECECH/2gAIAQIBAT8BiI7X/8QAFxAAAwEAAAAAAAAAAAAAAAAAAAExEP/aAAgBAQAGPwLYiIh//8QAGxAAAgIDAQAAAAAAAAAAAAAAAAERMVGB4fH/2gAIAQEAAT8hkoizDQ2NpWcUeUJdItH/2gAMAwEAAgADAAAAEKsP/8QAFhEBAQEAAAAAAAAAAAAAAAAAAREQ/9oACAEDAQE/EGEEz//EABYRAQEBAAAAAAAAAAAAAAAAAAEAEf/aAAgBAgEBPxB4rbS//8QAGhABAAMBAQEAAAAAAAAAAAAAAQAhURExgf/aAAgBAQABPxBUmqrixTQ9DY3K4HCLpUUukU9f5hvDsAT/2Q=='); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="Imag2Text"
        title="Imag2Text"
        src="/static/648a51197ffa44148d305aef4e2f4e9e/828fb/image26.jpg"
        srcset="/static/648a51197ffa44148d305aef4e2f4e9e/ff44c/image26.jpg 158w,
/static/648a51197ffa44148d305aef4e2f4e9e/a6688/image26.jpg 315w,
/static/648a51197ffa44148d305aef4e2f4e9e/828fb/image26.jpg 630w,
/static/648a51197ffa44148d305aef4e2f4e9e/0ede0/image26.jpg 945w,
/static/648a51197ffa44148d305aef4e2f4e9e/6a068/image26.jpg 960w"
        sizes="(max-width: 630px) 100vw, 630px"
        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"
        loading="lazy"
        decoding="async"
      />
  </a>
    </span></p>
<h3>12.2. Text to Image</h3>
<p>그렇다면 그 반대의 경우로 생각해보면, 설명하는 문장을 넣어주면 해당 이미지를 생성할 수 있지 않을까?</p>
<p>OpenAI에서 만든 이미지 생성 모델 : DALL-E</p>
<p>GPT를 만든 OpenAI에서 GPT의 생성기능을 이미지에 적용하여 이미지 생성 모델을 만들었다. 사용된 파라미터는 12B(120억)개이다</p>
<p><span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; "
    >
      <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 53.79746835443038%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAABYlAAAWJQFJUiTwAAACHklEQVQoz5WT3UtTYRzH99dENwVWFOFF6M3QkogRGBSF9CIWZcws6KIgCMLImwIj7QUTIzKworJS0jR15Vu+rNRtunm2nbPt7MydnXPc2faJnTWqm8wfPM/39/B8+fx+PC821olcLmdp2lxDjUdJyRJKXCIUE5CVCHIiSlDyY6zpls+2HjCbzVg6vTjBpcYKui5X88C5jyN126k8s4PaKwexHy9heLJ/Y8Dv3m/sP7WFtquHeFy/l6bachw1m3Gc3kb12VIm5kY3Bgx6Z7l1spS2c3ae3Gyg88Z5OpyVHK7aRZ1jK77/B2YtFb1uOhsdPL92jI6mC3Tfu85g60Xs5Xuose9EWhgvAIuHntfiyEOKeSZjWvuBH1M0H93NQ2cV9+sraD5Rxu3aMhoOlOCs3MTSzMjvDovQf0VSlhjrfsT4y3Zcr9pxdd1l9FkLX160MvD0DrGQvwAMh4U8EjkWQRD8pNM6nsV5ohERM20grCxbRtVQmVmZxRNaIBkX/yqmxiXM4rNZCsikdBAjGgFBQUnmWPCECYoq8UQGj08i3/9qLMy46w3z0wPE3GOQVEBdBU1Fdo9hRAtFbHNeGPqq0NMfoHcoxOs+Px+HI/QNibimVNw+UPPFZQl9cgLh0wDiyGd879+y3PsO34cegoP9mCv+P285jZZS0FMKZlrF0BLWmpxR/C/WrGUyyMkkqrHGqq6TMk0Smk5C08j+cv4Ej+oPKMPuPiYAAAAASUVORK5CYII='); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="토큰화 되어있는 이미지의 모습"
        title="토큰화 되어있는 이미지의 모습"
        src="/static/f6adce89d0c9a35f581ce1c9b078c539/f058b/image27.png"
        srcset="/static/f6adce89d0c9a35f581ce1c9b078c539/c26ae/image27.png 158w,
/static/f6adce89d0c9a35f581ce1c9b078c539/6bdcf/image27.png 315w,
/static/f6adce89d0c9a35f581ce1c9b078c539/f058b/image27.png 630w,
/static/f6adce89d0c9a35f581ce1c9b078c539/40601/image27.png 945w,
/static/f6adce89d0c9a35f581ce1c9b078c539/78612/image27.png 1260w,
/static/f6adce89d0c9a35f581ce1c9b078c539/aaf7f/image27.png 1668w"
        sizes="(max-width: 630px) 100vw, 630px"
        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"
        loading="lazy"
        decoding="async"
      />
    </span>
이미지출처: <a href="https://jiho-ml.com/weekly-nlp-40/">https://jiho-ml.com/weekly-nlp-40/</a>
<span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; "
    >
      <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 23.417721518987342%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAABYlAAAWJQFJUiTwAAAA80lEQVQY022Q3U7CQBSE+/5v45UXXpYIxKLVWIk1EbqUtssWCy1tQXb3UykhJXGSk5zfyZxxOMNa28U/+WmOxVhzqo3RtG3Lrqpomoa6rinLEq01jn3JLkcnmF6uz2TaQH9Fw2sQMByN8H2fyWSC67oURYGT3HhwtOQiZRUlmGrPIvxkkyrM7oCcx3CwNF8F0+CBj+kT6j2AsqbebFFK0Ycj0wxajZzFxOGMPEp4HIwJ/YC1yHjzntmvK7ZSMry/xRvfIbwBVhXkc0EkRKfadHY4f6/Yo+6a35rFPEJlkqWIWaUZMklZigXoKyH0Xbr4/Ev4A4I/fZIjuu8fAAAAAElFTkSuQmCC'); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="다음 픽셀 토큰 예측"
        title="다음 픽셀 토큰 예측"
        src="/static/57ba2cdd11c1d8072bc7c8aafe4f3bac/f058b/image28.png"
        srcset="/static/57ba2cdd11c1d8072bc7c8aafe4f3bac/c26ae/image28.png 158w,
/static/57ba2cdd11c1d8072bc7c8aafe4f3bac/6bdcf/image28.png 315w,
/static/57ba2cdd11c1d8072bc7c8aafe4f3bac/f058b/image28.png 630w,
/static/57ba2cdd11c1d8072bc7c8aafe4f3bac/40601/image28.png 945w,
/static/57ba2cdd11c1d8072bc7c8aafe4f3bac/78612/image28.png 1260w,
/static/57ba2cdd11c1d8072bc7c8aafe4f3bac/29007/image28.png 1600w"
        sizes="(max-width: 630px) 100vw, 630px"
        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"
        loading="lazy"
        decoding="async"
      />
    </span>
이미지출처: <a href="https://jiho-ml.com/weekly-nlp-40/">https://jiho-ml.com/weekly-nlp-40/</a></p>
<p>사진 설명 문장과 이미지 픽셀을 1차원화 한 벡터의 시작부분까지 입력으로 넣어주면, 이후 픽셀들을 하나씩 생성하여 주고, 결과를 2차원화 하면 생성된 이미지를 볼 수 있다.</p>
<p><span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; "
    >
      <a
    class="gatsby-resp-image-link"
    href="/static/75f86ddaeb7345479ddabff39c1cd1b0/6568d/image29.png"
    style="display: block"
    target="_blank"
    rel="noopener"
  >
    <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 43.0379746835443%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsSAAALEgHS3X78AAACVklEQVQoz12R20vTcRjGv7/f5iFnKXNbO+hKKS1FcHlg1izLSmaamanTnFMIS5OtTcNSS1FKl0aRDLQsC6PM7CB0IBPtwi7qRqKrMOhGL/ofik9sXohdfHjeF14eHp5XqFQqoqOjCQsLQ6vVYjKZSDCbQ2o0GkMaFxeHXq8PzQaDAbVajSQEsiwhSeuIIEEjpVKJEGIDsiwjSQJZCu4SkqxEVoaF9P/bDcTExKDRaNBoNbR3Xebq0AAdw7c5VVlFeWU1NQ2NlJYU4SjKxWm3UVFoxdtop7+vni5vGaUlRymtcFBT30Bmdg5CoVAQxGg08OP3Kn+Wf8Lofe4+mcLf3cu9zk5aPB6y9yWRU5WBvTyLpakelptqWJnp49nzMV5Nz/B59iPe9kvBlGtR9VodL5cW6Jns5pZrLye8Z8g6YGF/YSaJmakIt4JMdxSKBhW+DgcDGTsIDLqYnnvK2MU2xlsa6breG+xRChnqNFrG3k5Q9K6H43P92KrsxFtS0VnT2bkniZOdu1i0p1N8OgHZFoWwb0FYFeQ5jpCSnIY5PhFLvm3dUJNgZuXhCFQf4u+XRQZvjuBt6qTNN8RQdzsrj138euDig78cdWQ4h6OisIRHkpZlwuXNx+nLJz3bFHzgmqFKr2XwjpuZunT8kz0cexOg2V/PwdFWbK89nPu0m9b5FByTqcSaY/mqisQfsYna2m2MF8Qx4dyKu277eodicwRiuArx4jwiUI2YvYD45kYslKJ6X0Dudy3WVTWp8zosN5w0+yoo9pTRda2YQJGOR85krpzN4x/GlkWvU9/ecQAAAABJRU5ErkJggg=='); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="DALL-E의 생성결과 예"
        title="DALL-E의 생성결과 예"
        src="/static/75f86ddaeb7345479ddabff39c1cd1b0/f058b/image29.png"
        srcset="/static/75f86ddaeb7345479ddabff39c1cd1b0/c26ae/image29.png 158w,
/static/75f86ddaeb7345479ddabff39c1cd1b0/6bdcf/image29.png 315w,
/static/75f86ddaeb7345479ddabff39c1cd1b0/f058b/image29.png 630w,
/static/75f86ddaeb7345479ddabff39c1cd1b0/40601/image29.png 945w,
/static/75f86ddaeb7345479ddabff39c1cd1b0/78612/image29.png 1260w,
/static/75f86ddaeb7345479ddabff39c1cd1b0/6568d/image29.png 1660w"
        sizes="(max-width: 630px) 100vw, 630px"
        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"
        loading="lazy"
        decoding="async"
      />
  </a>
    </span>
DALL-E의 생성결과 예</p>
<p>OpenAI에서 만든 두번째 이미지 생성 모델 : GLIDE</p>
<p>GLIDE(Guided Language-to-Image Diffusion for Generation and Editing)는 유도 확산 기술을 적용하여 파라미터가 DALL-E의 1/3인 3.5B(35억)개만으로 고품질 이미지를 생성하는 모델을 만들었다.</p>
<p>이 기술은 이미지의 부분을 마스킹하여 원하는 이미지를 텍스트입력으로 만들어 낼 수 있는 능력에 특화되어 있다.</p>
<p>아래 이미지는 GLIDE의 실행 결과물 예제이다.</p>
<p><span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; "
    >
      <a
    class="gatsby-resp-image-link"
    href="/static/879977f3fd2d0810b6b3f7bfca1ee124/71c8e/image30.png"
    style="display: block"
    target="_blank"
    rel="noopener"
  >
    <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 32.911392405063296%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAIAAACHqfpvAAAACXBIWXMAAAsTAAALEwEAmpwYAAABtklEQVQY0wGrAVT+AJq9msrO1K6ys5mUkZ+bl8jR0q2xs5iTj6KfnMfP0a2xspaPi6ejo8fd0KzasJSTh62rq5mhpMPKzfz9/gCAsoHV19+rsLGAfn2MiYnS09yqrLB/fHqQj4/P0tiqr7B5dnWXlZfQ5Nio3a51eW+joqSQlpvIz9L+//8Adn1wjo2PeXh3gXt6gYN9iqeLfIJ5gXp7goOAjaeQfn58f3p4hoKDi5KLf5V9fIN2iYWHgYKAfnx76+npAEo5NGFTSldJPl9MSlJnPV6zSFd8P19KS1xdSHiSZV1ORFxOSF9US25gWV1TQ1hXRWJUTnBlW1FANuTh4ADc2NTo5ePo5OLm4d/U1c3Y39Lk6N7m4uDV0MzWz8/i3trl4d/X0s/W0s/o5ODi3tvY1NHa1tPn49/9/f0A9vf48fLy9PX1+Pn58/P16unr8fDy8/T18fLz7O3t7/Hx9/n48PHy6uvs9fb29/j48fLz7/Dw8vPz+fn5ANTU1NnY2NnY2NbW1dna2d3e3ujo6Ono6erq6urq6uvr6unq6enp6Ovr6+3t7erq6ezs7Ozs7Ojo5/P09BajKbYOnnsRAAAAAElFTkSuQmCC'); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="GLIDE의 실행 결과물 예제"
        title="GLIDE의 실행 결과물 예제"
        src="/static/879977f3fd2d0810b6b3f7bfca1ee124/f058b/image30.png"
        srcset="/static/879977f3fd2d0810b6b3f7bfca1ee124/c26ae/image30.png 158w,
/static/879977f3fd2d0810b6b3f7bfca1ee124/6bdcf/image30.png 315w,
/static/879977f3fd2d0810b6b3f7bfca1ee124/f058b/image30.png 630w,
/static/879977f3fd2d0810b6b3f7bfca1ee124/71c8e/image30.png 875w"
        sizes="(max-width: 630px) 100vw, 630px"
        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"
        loading="lazy"
        decoding="async"
      />
  </a>
    </span>
<span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 592px; "
    >
      <a
    class="gatsby-resp-image-link"
    href="/static/bcf728aaefac53872a774853020db684/1b853/image31.png"
    style="display: block"
    target="_blank"
    rel="noopener"
  >
    <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 135.44303797468356%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,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'); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="GLIDE의 실행 결과물 예제"
        title="GLIDE의 실행 결과물 예제"
        src="/static/bcf728aaefac53872a774853020db684/1b853/image31.png"
        srcset="/static/bcf728aaefac53872a774853020db684/c26ae/image31.png 158w,
/static/bcf728aaefac53872a774853020db684/6bdcf/image31.png 315w,
/static/bcf728aaefac53872a774853020db684/1b853/image31.png 592w"
        sizes="(max-width: 592px) 100vw, 592px"
        style="width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;"
        loading="lazy"
        decoding="async"
      />
  </a>
    </span>
GLIDE의 실행 결과물 예제</p>
<h2>13. kpfBERT (언론진흥재단 BERT)</h2>
<h3>13.1. 신문기사에 특화된 kpfBERT</h3>
<ul>
<li>BERT의 특징은 대량의 데이터를 사전학습을 하는 모델이다.</li>
<li>그렇기에 데이터 수집과 모델 트레이닝에 많은 비용과 노력이 발생한다.</li>
<li>또한 어떤 데이터의 집단이냐에 따라 학습된 모델에 편향성이 생길 수도 있다.</li>
<li>하지만 같은 개념으로 특정 데이터에만 적용되는 모델이라고 가정하면, 같은 계열의 데이터로 사전학습을 진행한 모델이 더 뛰어난 성능을 보일 수 있다.</li>
<li>한국언론진흥재단에서는 언론에 특화된 사전학습된 BERT모델을 생성함으로써 여러 언론 관련 산업에서 자연어처리 작업에 활용할 수 있는 기반을 만들었다.</li>
</ul>
<h3>13.2. kpfBERT의 성능비교</h3>
<p>5가지 BERT의 일반적인 태스크에 대한 비교 평가값. 동일한 하드웨어, 파이프라인을 통한 각 모델별 실측값이다.</p>
<div style="overflow-x:auto;">
<table>
  <thead>
    <tr>
      <th scope="col">
        구분
      </th>
      <th scope="col">
        NSMC
      </th>
      <th scope="col">
        KLUE-NLI
      </th>
      <th scope="col">
        KLUE-STS
      </th>
      <th scope="col">
        KorQuAD v1
      </th>
      <th scope="col">
        KLUE MRC
      </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th scope="row">데이터 특징 및 규격</th>
      <td>
        영화 리뷰 감정 분석<br/>
        학습: 150,000 문장<br />
        평가: 50,000 문장
      </td>
      <td>
        자연어 추론<br />
        학습: 24,998 문장<br />
        평가: 3,000 문장 (dev셋)
      </td>
      <td>
        문장의 의미적 유사도 측정<br />
        학습: 11,668 문장<br />
        평가: 519 문장 (dev셋)
      </td>
      <td>
        기계독해<br />
        학습: 60,406 건<br />
        평가: 5,774 건 (dev셋)
      </td>
      <td>
        기계 독해<br />
        학습: 17,554 건<br />
        평가: 5,841 건 (dev셋)
      </td>
    </tr>
    <tr>
      <th scope="row">
        평가방법
      </th>
      <td>
        accuracy
      </td>
      <td>
        accuracy
      </td>
      <td>
        Pearson Correlation
      </td>
      <td>
        Exact Match / F1
      </td>
      <td>
        Exact Match / Rouge W
      </td>
    </tr>
    <tr>
      <th scope="row">
        KPF BERT
      </th>
      <td>
        91.29%
      </td>
      <td>
        87.67%
      </td>
      <td>
        92.95%
      </td>
      <td>
        86.42% / 94.95%
      </td>
      <td>
        69.51% / 75.84%
      </td>
    </tr>
    <tr>
      <th scope="row">
        KLUE BERT
      </th>
      <td>
        90.62%
      </td>
      <td>
        81.33%
      </td>
      <td>
        91.14%
      </td>
      <td>
        83.84% / 93.23%
      </td>
      <td>
        61.91% / 68.38%
      </td>
    </tr>
    <tr>
      <th scope="row">
        KorBERT(ETRI)
      </th>
      <td>
        90.46%
      </td>
      <td>
        80.56%
      </td>
      <td>
        89.52%
      </td>
      <td>
        20.11%&nbsp;/ 82.00%
      </td>
      <td>
        30.56%&nbsp;/ 58.59%
      </td>
    </tr>
    <tr>
      <th scope="row">
        KoBERT(SKT)
      </th>
      <td>
        89.92%
      </td>
      <td>
        79.53%
      </td>
      <td>
        86.17%
      </td>
      <td>
        16.85%&nbsp;/ 71.36%
      </td>
      <td>
        28.56%&nbsp;/ 42.06%
      </td>
    </tr>
    <tr>
      <th scope="row">
        BERT base multilingual
      </th>
      <td>
        87.33%
      </td>
      <td>
        73.30%
      </td>
      <td>
        85.66%
      </td>
      <td>
        69.10% / 90.02%
      </td>
      <td>
        44.58% / 55.92%
      </td>
    </tr>
  </tbody>
</table>
</div>
다른 한글BERT에 비해 성능이 우수하게 나오는 것은 RoBERTa, ELECTRA등의 기술들이 적극적으로 반영되었고, 다량의 양질의 학습데이터에 의한 결과로 예상된다.
<h3>13.3. kpfBERT의 활용</h3>
<ol>
<li>뉴스 본문 요약 <a href="https://github.com/KPFBERT/kpfbertsum">https://github.com/KPFBERT/kpfbertsum</a>
<ul>
<li>뉴스 본문의 중요문장을 찾아서 3문장으로 제시하는 모델이다. 본문의 3줄요약은 모바일 시대에 적절한 정보전달 방법이다. 다만 BERT가 생성형 모델이 아니라 본문내용을 요약해서 문장을 생성하는 태스크는 완성도가 많이 떨어진다. 대신 문장 분석력을 사용하여 전체 문장 중 가장 중요도가 높은 문장 3개를 추천하는 방식으로 본문내용을 요약표현하는 활용법이다.</li>
</ul>
</li>
<li>kpf-SBERT <a href="https://github.com/KPFBERT/kpfSBERT">https://github.com/KPFBERT/kpfSBERT</a>
<ul>
<li>Sentence BERT를 제작하는 방법에 대한 소스코드이다. 단어별 임베딩이 아닌 문장 전체를 하나의 동일한 크기의 임베딩 벡터로 변환한다. 이것은 문장간의 비교를 BERT대비 엄청나게 빠르고 효율적으로 연산할 수 있다. 문장의 의미적 유사도를 쉽게 수치로 비교할 수 있고, 미세하게 다른표현이지만 중복되는 문장 또는 기사를 찾아내어 중복제거 등의 태스크에 활용할 수 있다.</li>
</ul>
</li>
<li>kpf-SBERT를 이용한 뉴스 클러스터링 <a href="https://github.com/KPFBERT/kpfSBERT_Clustering">https://github.com/KPFBERT/kpfSBERT_Clustering</a>
<ul>
<li>위에서 제작한 kpf-SBERT를 이용하여 HDBSCAN으로 뉴스를 자동 클러스터링 하는 모델의 예제이다. 소스코드에서는 기사의 제목만으로 의미적 유사도에 기반하여 DBSCAN보다 성능이 좋은 HDBSCAN을 이용하여 클러스터링 하였다.</li>
</ul>
</li>
<li>추가적으로 활용해 볼 수 있는 영역
<ul>
<li>맞춤법 검사기: 단순 맞춤법 검사를 넘어 문맥과 의미를 고려한 맞춤법 검사기</li>
<li>단어 자동완성: 입력하는 연속된 문장에서 해당시점에 가장 적절한 단어를 추천하는 모델</li>
<li>문장의 어색한 표현이나 어휘 체크: 1차 완성된 기사에서 문장간 또는 문장내 어울리지 않는 표현이나 어휘, 잘못된 문법적 오류 검출에 활용 가능</li>
<li>혐오표현 순화: 뉴스 댓글 등에서의 혐오표현을 검출하고 순화해서 표현하는 모델</li>
<li>기사의 논조 분석: 중심 사안에 대한 기사의 긍정, 부정 등의 논조 파악 모델</li>
<li>광고성 기사 검출: 협찬기사, 정보전달을 가장한 광고 등의 광고성 기사 검출 모델</li>
</ul>
</li>
</ol></section><hr/><footer><div class="bio"><img class="bio-avatar" src="../images/Justa.png" width="50" height="50" alt=""/><p>작성자 <strong>Justa</strong></p></div></footer></article><nav class="blog-post-nav"><ul style="display:flex;flex-wrap:wrap;justify-content:space-between;list-style:none;padding:0"><li><a rel="prev" href="/toysmyth/">← <!-- -->토이스미스</a></li><li><a rel="next" href="/2022-10-24-ai-for-creation/">머지않은 미래, 인공지능이 창작의 주체가 될 수 있을까?<!-- --> →</a></li></ul></nav></main><footer><p class="copyright">© MediaNavi</p></footer></div></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/2022-01-21-history-of-ai/";window.___webpackCompilationHash="f8caf2253b1a6c7714c9";/*]]>*/</script><script id="gatsby-chunk-mapping">/*<![CDATA[*/window.___chunkMapping={"app":["/app-90c1351ed6738eeedfbb.js"],"component---src-pages-404-js":["/component---src-pages-404-js-ac989e24304a34f67e47.js"],"component---src-pages-index-js":["/component---src-pages-index-js-2c4d359f6f501ded91cb.js"],"component---src-pages-works-js":["/component---src-pages-works-js-beb11e784d78eadba7dc.js"],"component---src-templates-blog-post-js":["/component---src-templates-blog-post-js-3b9ccc2046bedbfe9991.js"]};/*]]>*/</script><script src="/app-90c1351ed6738eeedfbb.js" async=""></script><script src="/framework-1e99f3a671b040f20568.js" async=""></script><script src="/webpack-runtime-f59dd854d445c0e46e1c.js" async=""></script></body></html>